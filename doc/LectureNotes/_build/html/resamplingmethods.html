

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>12. Resampling Techniques, Bootstrap and Blocking &#8212; Quantum Mechanics for Many-Particle Systems</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'resamplingmethods';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="13. Boltzmann Machines" href="boltzmannmachines.html" />
    <link rel="prev" title="11. Gradient Methods" href="gradientmethods.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Quantum mechanics for many-particle systems
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">About the course</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="schedule.html">Schedule</a></li>
<li class="toctree-l1"><a class="reference internal" href="teachers.html">Instructor information</a></li>
<li class="toctree-l1"><a class="reference internal" href="textbooks.html">Textbooks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Basics of Many-Body Physics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="notation.html">1. Introduction to many-body physics</a></li>
<li class="toctree-l1"><a class="reference internal" href="secondquant.html">2. Introduction to  second quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="fci.html">3. Full configuration interaction theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="hfock.html">4. Hartree-Fock theory</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Post Hartree-Fock Methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mbpt.html">6. Many-body perturbation theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="cc.html">7. The Coupled Cluster Method</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Stochastic Methods</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="vmcdmc.html">10. Variational Monte Carlo methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradientmethods.html">11. Gradient Methods</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">12. Resampling Techniques, Bootstrap and Blocking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="boltzmannmachines.html">13. Boltzmann Machines</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computational Aspects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="vectorization.html">14. Optimization and Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallelization.html">15. Parallelization with MPI and OpenMPI</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/resamplingmethods.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/resamplingmethods.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Resampling Techniques, Bootstrap and Blocking</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-resampling-methods">12.1. Why resampling methods ?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-analysis">12.2. Statistical analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics-wrapping-up-from-last-week">12.3. Statistics, wrapping up from last week</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics-final-expression">12.4. Statistics, final expression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics-effective-number-of-correlations">12.5. Statistics, effective number of correlations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#can-we-understand-this-time-auto-correlation-function">12.6. Can we understand this? Time Auto-correlation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-auto-correlation-function">12.7. Time Auto-correlation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">12.8. Time Auto-correlation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">12.9. Time Auto-correlation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">12.10. Time Auto-correlation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">12.11. Time Auto-correlation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-time">12.12. Correlation Time</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-jackknife-and-bootstrap">12.13. Resampling methods: Jackknife and Bootstrap</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-jackknife">12.14. Resampling methods: Jackknife</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-jackknife-estimator">12.15. Resampling methods: Jackknife estimator</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jackknife-code-example">12.16. Jackknife code example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-bootstrap">12.17. Resampling methods: Bootstrap</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-bootstrap-background">12.18. Resampling methods: Bootstrap background</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-more-bootstrap-background">12.19. Resampling methods: More Bootstrap background</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-bootstrap-approach">12.20. Resampling methods: Bootstrap approach</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-bootstrap-steps">12.21. Resampling methods: Bootstrap steps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-example-for-the-bootstrap-method">12.22. Code example for the Bootstrap method</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-blocking">12.23. Resampling methods: Blocking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#blocking-transformations">12.24. Blocking Transformations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">12.25. Blocking Transformations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#blocking-transformations-getting-there">12.26. Blocking Transformations, getting there</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#blocking-transformations-final-expressions">12.27. Blocking Transformations, final expressions</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="resampling-techniques-bootstrap-and-blocking">
<h1><span class="section-number">12. </span>Resampling Techniques, Bootstrap and Blocking<a class="headerlink" href="#resampling-techniques-bootstrap-and-blocking" title="Permalink to this heading">#</a></h1>
<section id="why-resampling-methods">
<h2><span class="section-number">12.1. </span>Why resampling methods ?<a class="headerlink" href="#why-resampling-methods" title="Permalink to this heading">#</a></h2>
<p><strong>Statistical analysis.</strong></p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>* Our simulations can be treated as *computer experiments*. This is particularly the case for Monte Carlo methods

* The results can be analysed with the same statistical tools as we would use analysing experimental data.

* As in all experiments, we are looking for expectation values and an estimate of how accurate they are, i.e., possible sources for errors.
</pre></div>
</div>
</section>
<section id="statistical-analysis">
<h2><span class="section-number">12.2. </span>Statistical analysis<a class="headerlink" href="#statistical-analysis" title="Permalink to this heading">#</a></h2>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>* As in other experiments, many numerical  experiments have two classes of errors:

  * Statistical errors

  * Systematical errors


* Statistical errors can be estimated using standard tools from statistics

* Systematical errors are method specific and must be treated differently from case to case.
</pre></div>
</div>
</section>
<section id="statistics-wrapping-up-from-last-week">
<h2><span class="section-number">12.3. </span>Statistics, wrapping up from last week<a class="headerlink" href="#statistics-wrapping-up-from-last-week" title="Permalink to this heading">#</a></h2>
<p>Let us analyze the problem by splitting up the correlation term into
partial sums of the form:</p>
<div class="math notranslate nohighlight">
\[
f_d = \frac{1}{n-d}\sum_{k=1}^{n-d}(x_k - \bar x_n)(x_{k+d} - \bar x_n)
\]</div>
<p>The correlation term of the error can now be rewritten in terms of
<span class="math notranslate nohighlight">\(f_d\)</span></p>
<div class="math notranslate nohighlight">
\[
\frac{2}{n}\sum_{k&lt;l} (x_k - \bar x_n)(x_l - \bar x_n) =
2\sum_{d=1}^{n-1} f_d
\]</div>
<p>The value of <span class="math notranslate nohighlight">\(f_d\)</span> reflects the correlation between measurements
separated by the distance <span class="math notranslate nohighlight">\(d\)</span> in the sample samples.  Notice that for
<span class="math notranslate nohighlight">\(d=0\)</span>, <span class="math notranslate nohighlight">\(f\)</span> is just the sample variance, <span class="math notranslate nohighlight">\(\mathrm{var}(x)\)</span>. If we divide <span class="math notranslate nohighlight">\(f_d\)</span>
by <span class="math notranslate nohighlight">\(\mathrm{var}(x)\)</span>, we arrive at the so called <em>autocorrelation function</em></p>
<div class="math notranslate nohighlight">
\[
\kappa_d = \frac{f_d}{\mathrm{var}(x)}
\]</div>
<p>which gives us a useful measure of pairwise correlations
starting always at <span class="math notranslate nohighlight">\(1\)</span> for <span class="math notranslate nohighlight">\(d=0\)</span>.</p>
</section>
<section id="statistics-final-expression">
<h2><span class="section-number">12.4. </span>Statistics, final expression<a class="headerlink" href="#statistics-final-expression" title="Permalink to this heading">#</a></h2>
<p>The sample error can now be
written in terms of the autocorrelation function:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{err}_X^2 =
\frac{1}{n}\mathrm{var}(x)+\frac{2}{n}\cdot\mathrm{var}(x)\sum_{d=1}^{n-1}
\frac{f_d}{\mathrm{var}(x)}\nonumber
\]</div>
<div class="math notranslate nohighlight">
\[
=
\left(1+2\sum_{d=1}^{n-1}\kappa_d\right)\frac{1}{n}\mathrm{var}(x)\nonumber
\]</div>
<!-- Equation labels as ordinary links -->
<div id="_auto1"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} 
=\frac{\tau}{n}\cdot\mathrm{var}(x)
\label{_auto1} \tag{1}
\end{equation}
\]</div>
<p>and we see that <span class="math notranslate nohighlight">\(\mathrm{err}_X\)</span> can be expressed in terms the
uncorrelated sample variance times a correction factor <span class="math notranslate nohighlight">\(\tau\)</span> which
accounts for the correlation between measurements. We call this
correction factor the <em>autocorrelation time</em>:</p>
<!-- Equation labels as ordinary links -->
<div id="eq:autocorrelation_time"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\tau = 1+2\sum_{d=1}^{n-1}\kappa_d
\label{eq:autocorrelation_time} \tag{2}
\end{equation}
\]</div>
</section>
<section id="statistics-effective-number-of-correlations">
<h2><span class="section-number">12.5. </span>Statistics, effective number of correlations<a class="headerlink" href="#statistics-effective-number-of-correlations" title="Permalink to this heading">#</a></h2>
<p>For a correlation free experiment, <span class="math notranslate nohighlight">\(\tau\)</span>
equals 1.</p>
<p>We can interpret a sequential
correlation as an effective reduction of the number of measurements by
a factor <span class="math notranslate nohighlight">\(\tau\)</span>. The effective number of measurements becomes:</p>
<div class="math notranslate nohighlight">
\[
n_\mathrm{eff} = \frac{n}{\tau}
\]</div>
<p>To neglect the autocorrelation time <span class="math notranslate nohighlight">\(\tau\)</span> will always cause our
simple uncorrelated estimate of <span class="math notranslate nohighlight">\(\mathrm{err}_X^2\approx \mathrm{var}(x)/n\)</span> to
be less than the true sample error. The estimate of the error will be
too <em>good</em>. On the other hand, the calculation of the full
autocorrelation time poses an efficiency problem if the set of
measurements is very large.</p>
</section>
<section id="can-we-understand-this-time-auto-correlation-function">
<h2><span class="section-number">12.6. </span>Can we understand this? Time Auto-correlation Function<a class="headerlink" href="#can-we-understand-this-time-auto-correlation-function" title="Permalink to this heading">#</a></h2>
<p>The so-called time-displacement autocorrelation <span class="math notranslate nohighlight">\(\phi(t)\)</span> for a quantity <span class="math notranslate nohighlight">\(\mathbf{M}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\phi(t) = \int dt' \left[\mathbf{M}(t')-\langle \mathbf{M} \rangle\right]\left[\mathbf{M}(t'+t)-\langle \mathbf{M} \rangle\right],
\]</div>
<p>which can be rewritten as</p>
<div class="math notranslate nohighlight">
\[
\phi(t) = \int dt' \left[\mathbf{M}(t')\mathbf{M}(t'+t)-\langle \mathbf{M} \rangle^2\right],
\]</div>
<p>where <span class="math notranslate nohighlight">\(\langle \mathbf{M} \rangle\)</span> is the average value and
<span class="math notranslate nohighlight">\(\mathbf{M}(t)\)</span> its instantaneous value. We can discretize this function as follows, where we used our
set of computed values <span class="math notranslate nohighlight">\(\mathbf{M}(t)\)</span> for a set of discretized times (our Monte Carlo cycles corresponding to moving all electrons?)</p>
<!-- Equation labels as ordinary links -->
<div id="eq:phitf"></div>
<div class="math notranslate nohighlight">
\[
\phi(t)  = \frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}\mathbf{M}(t')\mathbf{M}(t'+t)
-\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}\mathbf{M}(t')\times
\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}\mathbf{M}(t'+t).
\label{eq:phitf} \tag{3}
\]</div>
</section>
<section id="time-auto-correlation-function">
<h2><span class="section-number">12.7. </span>Time Auto-correlation Function<a class="headerlink" href="#time-auto-correlation-function" title="Permalink to this heading">#</a></h2>
<p>One should be careful with times close to <span class="math notranslate nohighlight">\(t_{\mathrm{max}}\)</span>, the upper limit of the sums
becomes small and we end up integrating over a rather small time interval. This means that the statistical
error in <span class="math notranslate nohighlight">\(\phi(t)\)</span> due to the random nature of the fluctuations in <span class="math notranslate nohighlight">\(\mathbf{M}(t)\)</span> can become large.</p>
<p>One should therefore choose <span class="math notranslate nohighlight">\(t \ll t_{\mathrm{max}}\)</span>.</p>
<p>Note that the variable <span class="math notranslate nohighlight">\(\mathbf{M}\)</span> can be any expectation values of interest.</p>
<p>The time-correlation function gives a measure of the correlation between the various values of the variable
at a time <span class="math notranslate nohighlight">\(t'\)</span> and a time <span class="math notranslate nohighlight">\(t'+t\)</span>. If we multiply the values of <span class="math notranslate nohighlight">\(\mathbf{M}\)</span> at these two different times,
we will get a positive contribution if they are fluctuating in the same direction, or a negative value
if they fluctuate in the opposite direction. If we then integrate over time, or use the discretized version of, the time correlation function <span class="math notranslate nohighlight">\(\phi(t)\)</span> should take a non-zero value if the fluctuations are
correlated, else it should gradually go to zero. For times a long way apart
the different values of <span class="math notranslate nohighlight">\(\mathbf{M}\)</span>  are most likely
uncorrelated and <span class="math notranslate nohighlight">\(\phi(t)\)</span> should be zero.</p>
</section>
<section id="id1">
<h2><span class="section-number">12.8. </span>Time Auto-correlation Function<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>We can derive the correlation time by observing that our Metropolis algorithm is based on a random
walk in the space of all  possible spin configurations.
Our probability
distribution function <span class="math notranslate nohighlight">\(\mathbf{\hat{w}}(t)\)</span> after a given number of time steps <span class="math notranslate nohighlight">\(t\)</span> could be written as</p>
<div class="math notranslate nohighlight">
\[
\mathbf{\hat{w}}(t) = \mathbf{\hat{W}^t\hat{w}}(0),
\]</div>
<p>with <span class="math notranslate nohighlight">\(\mathbf{\hat{w}}(0)\)</span> the distribution at <span class="math notranslate nohighlight">\(t=0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{\hat{W}}\)</span> representing the
transition probability matrix.
We can always expand <span class="math notranslate nohighlight">\(\mathbf{\hat{w}}(0)\)</span> in terms of the right eigenvectors of
<span class="math notranslate nohighlight">\(\mathbf{\hat{v}}\)</span> of <span class="math notranslate nohighlight">\(\mathbf{\hat{W}}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\mathbf{\hat{w}}(0)  = \sum_i\alpha_i\mathbf{\hat{v}}_i,
\]</div>
<p>resulting in</p>
<div class="math notranslate nohighlight">
\[
\mathbf{\hat{w}}(t) = \mathbf{\hat{W}}^t\mathbf{\hat{w}}(0)=\mathbf{\hat{W}}^t\sum_i\alpha_i\mathbf{\hat{v}}_i=
\sum_i\lambda_i^t\alpha_i\mathbf{\hat{v}}_i,
\]</div>
<p>with <span class="math notranslate nohighlight">\(\lambda_i\)</span> the <span class="math notranslate nohighlight">\(i^{\mathrm{th}}\)</span> eigenvalue corresponding to<br />
the eigenvector <span class="math notranslate nohighlight">\(\mathbf{\hat{v}}_i\)</span>.</p>
</section>
<section id="id2">
<h2><span class="section-number">12.9. </span>Time Auto-correlation Function<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h2>
<p>If we assume that <span class="math notranslate nohighlight">\(\lambda_0\)</span> is the largest eigenvector we see that in the limit <span class="math notranslate nohighlight">\(t\rightarrow \infty\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{\hat{w}}(t)\)</span> becomes proportional to the corresponding eigenvector
<span class="math notranslate nohighlight">\(\mathbf{\hat{v}}_0\)</span>. This is our steady state or final distribution.</p>
<p>We can relate this property to an observable like the mean energy.
With the probabilty <span class="math notranslate nohighlight">\(\mathbf{\hat{w}}(t)\)</span> (which in our case is the squared trial wave function) we
can write the expectation values as</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{M}(t) \rangle  = \sum_{\mu} \mathbf{\hat{w}}(t)_{\mu}\mathbf{M}_{\mu},
\]</div>
<p>or as the scalar of a  vector product</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{M}(t) \rangle  = \mathbf{\hat{w}}(t)\mathbf{m},
\]</div>
<p>with <span class="math notranslate nohighlight">\(\mathbf{m}\)</span> being the vector whose elements are the values of <span class="math notranslate nohighlight">\(\mathbf{M}_{\mu}\)</span> in its
various microstates <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
</section>
<section id="id3">
<h2><span class="section-number">12.10. </span>Time Auto-correlation Function<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h2>
<p>We rewrite this relation  as</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{M}(t) \rangle  = \mathbf{\hat{w}}(t)\mathbf{m}=\sum_i\lambda_i^t\alpha_i\mathbf{\hat{v}}_i\mathbf{m}_i.
\]</div>
<p>If we define <span class="math notranslate nohighlight">\(m_i=\mathbf{\hat{v}}_i\mathbf{m}_i\)</span> as the expectation value of
<span class="math notranslate nohighlight">\(\mathbf{M}\)</span> in the <span class="math notranslate nohighlight">\(i^{\mathrm{th}}\)</span> eigenstate we can rewrite the last equation as</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{M}(t) \rangle  = \sum_i\lambda_i^t\alpha_im_i.
\]</div>
<p>Since we have that in the limit <span class="math notranslate nohighlight">\(t\rightarrow \infty\)</span> the mean value is dominated by the
the largest eigenvalue <span class="math notranslate nohighlight">\(\lambda_0\)</span>, we can rewrite the last equation as</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{M}(t) \rangle  = \langle \mathbf{M}(\infty) \rangle+\sum_{i\ne 0}\lambda_i^t\alpha_im_i.
\]</div>
<p>We define the quantity</p>
<div class="math notranslate nohighlight">
\[
\tau_i=-\frac{1}{log\lambda_i},
\]</div>
<p>and rewrite the last expectation value as</p>
<!-- Equation labels as ordinary links -->
<div id="eq:finalmeanm"></div>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{M}(t) \rangle  = \langle \mathbf{M}(\infty) \rangle+\sum_{i\ne 0}\alpha_im_ie^{-t/\tau_i}.
\label{eq:finalmeanm} \tag{4}
\]</div>
</section>
<section id="id4">
<h2><span class="section-number">12.11. </span>Time Auto-correlation Function<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h2>
<p>The quantities <span class="math notranslate nohighlight">\(\tau_i\)</span> are the correlation times for the system. They control also the auto-correlation function
discussed above.  The longest correlation time is obviously given by the second largest
eigenvalue <span class="math notranslate nohighlight">\(\tau_1\)</span>, which normally defines the correlation time discussed above. For large times, this is the
only correlation time that survives. If higher eigenvalues of the transition matrix are well separated from
<span class="math notranslate nohighlight">\(\lambda_1\)</span> and we simulate long enough,  <span class="math notranslate nohighlight">\(\tau_1\)</span> may well define the correlation time.
In other cases we may not be able to extract a reliable result for <span class="math notranslate nohighlight">\(\tau_1\)</span>.
Coming back to the time correlation function <span class="math notranslate nohighlight">\(\phi(t)\)</span> we can present a more general definition in terms
of the mean magnetizations <span class="math notranslate nohighlight">\( \langle \mathbf{M}(t) \rangle\)</span>. Recalling that the mean value is equal
to <span class="math notranslate nohighlight">\( \langle \mathbf{M}(\infty) \rangle\)</span> we arrive at the expectation values</p>
<div class="math notranslate nohighlight">
\[
\phi(t) =\langle \mathbf{M}(0)-\mathbf{M}(\infty)\rangle \langle \mathbf{M}(t)-\mathbf{M}(\infty)\rangle,
\]</div>
<p>resulting in</p>
<div class="math notranslate nohighlight">
\[
\phi(t) =\sum_{i,j\ne 0}m_i\alpha_im_j\alpha_je^{-t/\tau_i},
\]</div>
<p>which is appropriate for all times.</p>
</section>
<section id="correlation-time">
<h2><span class="section-number">12.12. </span>Correlation Time<a class="headerlink" href="#correlation-time" title="Permalink to this heading">#</a></h2>
<p>If the correlation function decays exponentially</p>
<div class="math notranslate nohighlight">
\[
\phi (t) \sim \exp{(-t/\tau)}
\]</div>
<p>then the exponential correlation time can be computed as the average</p>
<div class="math notranslate nohighlight">
\[
\tau_{\mathrm{exp}}  =  -\langle  \frac{t}{log|\frac{\phi(t)}{\phi(0)}|} \rangle.
\]</div>
<p>If the decay is exponential, then</p>
<div class="math notranslate nohighlight">
\[
\int_0^{\infty} dt \phi(t)  = \int_0^{\infty} dt \phi(0)\exp{(-t/\tau)}  = \tau \phi(0),
\]</div>
<p>which  suggests another measure of correlation</p>
<div class="math notranslate nohighlight">
\[
\tau_{\mathrm{int}} = \sum_k \frac{\phi(k)}{\phi(0)},
\]</div>
<p>called the integrated correlation time.</p>
</section>
<section id="resampling-methods-jackknife-and-bootstrap">
<h2><span class="section-number">12.13. </span>Resampling methods: Jackknife and Bootstrap<a class="headerlink" href="#resampling-methods-jackknife-and-bootstrap" title="Permalink to this heading">#</a></h2>
<p>Two famous
resampling methods are the <strong>independent bootstrap</strong> and <strong>the jackknife</strong>.</p>
<p>The jackknife is a special case of the independent bootstrap. Still, the jackknife was made
popular prior to the independent bootstrap. And as the popularity of
the independent bootstrap soared, new variants, such as <strong>the dependent bootstrap</strong>.</p>
<p>The Jackknife and independent bootstrap work for
independent, identically distributed random variables.
If these conditions are not
satisfied, the methods will fail.  Yet, it should be said that if the data are
independent, identically distributed, and we only want to estimate the
variance of <span class="math notranslate nohighlight">\(\overline{X}\)</span> (which often is the case), then there is no
need for bootstrapping.</p>
</section>
<section id="resampling-methods-jackknife">
<h2><span class="section-number">12.14. </span>Resampling methods: Jackknife<a class="headerlink" href="#resampling-methods-jackknife" title="Permalink to this heading">#</a></h2>
<p>The Jackknife works by making many replicas of the estimator <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span>.
The jackknife is a resampling method, we explained that this happens by scrambling the data in some way. When using the jackknife, this is done by systematically leaving out one observation from the vector of observed values <span class="math notranslate nohighlight">\(\hat{x} = (x_1,x_2,\cdots,X_n)\)</span>.
Let <span class="math notranslate nohighlight">\(\hat{x}_i\)</span> denote the vector</p>
<div class="math notranslate nohighlight">
\[
\hat{x}_i = (x_1,x_2,\cdots,x_{i-1},x_{i+1},\cdots,x_n),
\]</div>
<p>which equals the vector <span class="math notranslate nohighlight">\(\hat{x}\)</span> with the exception that observation
number <span class="math notranslate nohighlight">\(i\)</span> is left out. Using this notation, define
<span class="math notranslate nohighlight">\(\widehat{\theta}_i\)</span> to be the estimator
<span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> computed using <span class="math notranslate nohighlight">\(\vec{X}_i\)</span>.</p>
</section>
<section id="resampling-methods-jackknife-estimator">
<h2><span class="section-number">12.15. </span>Resampling methods: Jackknife estimator<a class="headerlink" href="#resampling-methods-jackknife-estimator" title="Permalink to this heading">#</a></h2>
<p>To get an estimate for the bias and
standard error of <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span>, use the following
estimators for each component of <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span></p>
<div class="math notranslate nohighlight">
\[
\widehat{\mathrm{Bias}}(\widehat \theta,\theta) = (n-1)\left( - \widehat{\theta} + \frac{1}{n}\sum_{i=1}^{n} \widehat \theta_i \right) \qquad \text{and} \qquad \widehat{\sigma}^2_{\widehat{\theta} } = \frac{n-1}{n}\sum_{i=1}^{n}( \widehat{\theta}_i - \frac{1}{n}\sum_{j=1}^{n}\widehat \theta_j )^2.
\]</div>
</section>
<section id="jackknife-code-example">
<h2><span class="section-number">12.16. </span>Jackknife code example<a class="headerlink" href="#jackknife-code-example" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">randint</span><span class="p">,</span> <span class="n">randn</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>

<span class="k">def</span> <span class="nf">jackknife</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">stat</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">);</span><span class="n">t</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">);</span> <span class="n">inds</span> <span class="o">=</span> <span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">);</span> <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
    <span class="c1">## &#39;jackknifing&#39; by leaving out an observation for each i                                                                                                                      </span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">t</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">stat</span><span class="p">(</span><span class="n">delete</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="p">)</span>

    <span class="c1"># analysis                                                                                                                                                                     </span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Runtime: </span><span class="si">%g</span><span class="s2"> sec&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t0</span><span class="p">));</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Jackknife Statistics :&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;original           bias      std. error&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%8g</span><span class="s2"> </span><span class="si">%14g</span><span class="s2"> </span><span class="si">%15g</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">stat</span><span class="p">(</span><span class="n">data</span><span class="p">),(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">mean</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">var</span><span class="p">(</span><span class="n">t</span><span class="p">))</span><span class="o">**</span><span class="mf">.5</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">t</span>


<span class="c1"># Returns mean of data samples                                                                                                                                                     </span>
<span class="k">def</span> <span class="nf">stat</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>


<span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">15</span>
<span class="n">datapoints</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">datapoints</span><span class="p">)</span>
<span class="c1"># jackknife returns the data sample                                                                                                                                                </span>
<span class="n">t</span> <span class="o">=</span> <span class="n">jackknife</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Runtime: 0.0883594 sec
Jackknife Statistics :
original           bias      std. error
 100.049        100.039        0.149751
</pre></div>
</div>
</div>
</div>
</section>
<section id="resampling-methods-bootstrap">
<h2><span class="section-number">12.17. </span>Resampling methods: Bootstrap<a class="headerlink" href="#resampling-methods-bootstrap" title="Permalink to this heading">#</a></h2>
<p>Bootstrapping is a nonparametric approach to statistical inference
that substitutes computation for more traditional distributional
assumptions and asymptotic results. Bootstrapping offers a number of
advantages:</p>
<ol class="arabic simple">
<li><p>The bootstrap is quite general, although there are some cases in which it fails.</p></li>
<li><p>Because it does not require distributional assumptions (such as normally distributed errors), the bootstrap can provide more accurate inferences when the data are not well behaved or when the sample size is small.</p></li>
<li><p>It is possible to apply the bootstrap to statistics with sampling distributions that are difficult to derive, even asymptotically.</p></li>
<li><p>It is relatively simple to apply the bootstrap to complex data-collection plans (such as stratified and clustered samples).</p></li>
</ol>
</section>
<section id="resampling-methods-bootstrap-background">
<h2><span class="section-number">12.18. </span>Resampling methods: Bootstrap background<a class="headerlink" href="#resampling-methods-bootstrap-background" title="Permalink to this heading">#</a></h2>
<p>Since <span class="math notranslate nohighlight">\(\widehat{\theta} = \widehat{\theta}(\hat{X})\)</span> is a function of random variables,
<span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> itself must be a random variable. Thus it has
a pdf, call this function <span class="math notranslate nohighlight">\(p(\hat{t})\)</span>. The aim of the bootstrap is to
estimate <span class="math notranslate nohighlight">\(p(\hat{t})\)</span> by the relative frequency of
<span class="math notranslate nohighlight">\(\widehat{\theta}\)</span>. You can think of this as using a histogram
in the place of <span class="math notranslate nohighlight">\(p(\hat{t})\)</span>. If the relative frequency closely
resembles <span class="math notranslate nohighlight">\(p(\vec{t})\)</span>, then using numerics, it is straight forward to
estimate all the interesting parameters of <span class="math notranslate nohighlight">\(p(\hat{t})\)</span> using point
estimators.</p>
</section>
<section id="resampling-methods-more-bootstrap-background">
<h2><span class="section-number">12.19. </span>Resampling methods: More Bootstrap background<a class="headerlink" href="#resampling-methods-more-bootstrap-background" title="Permalink to this heading">#</a></h2>
<p>In the case that <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> has
more than one component, and the components are independent, we use the
same estimator on each component separately.  If the probability
density function of <span class="math notranslate nohighlight">\(X_i\)</span>, <span class="math notranslate nohighlight">\(p(x)\)</span>, had been known, then it would have
been straight forward to do this by:</p>
<ol class="arabic simple">
<li><p>Drawing lots of numbers from <span class="math notranslate nohighlight">\(p(x)\)</span>, suppose we call one such set of numbers <span class="math notranslate nohighlight">\((X_1^*, X_2^*, \cdots, X_n^*)\)</span>.</p></li>
<li><p>Then using these numbers, we could compute a replica of <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> called <span class="math notranslate nohighlight">\(\widehat{\theta}^*\)</span>.</p></li>
</ol>
<p>By repeated use of (1) and (2), many
estimates of <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> could have been obtained. The
idea is to use the relative frequency of <span class="math notranslate nohighlight">\(\widehat{\theta}^*\)</span>
(think of a histogram) as an estimate of <span class="math notranslate nohighlight">\(p(\hat{t})\)</span>.</p>
</section>
<section id="resampling-methods-bootstrap-approach">
<h2><span class="section-number">12.20. </span>Resampling methods: Bootstrap approach<a class="headerlink" href="#resampling-methods-bootstrap-approach" title="Permalink to this heading">#</a></h2>
<p>But
unless there is enough information available about the process that
generated <span class="math notranslate nohighlight">\(X_1,X_2,\cdots,X_n\)</span>, <span class="math notranslate nohighlight">\(p(x)\)</span> is in general
unknown. Therefore, <a class="reference external" href="https://projecteuclid.org/euclid.aos/1176344552">Efron in 1979</a>  asked the
question: What if we replace <span class="math notranslate nohighlight">\(p(x)\)</span> by the relative frequency
of the observation <span class="math notranslate nohighlight">\(X_i\)</span>; if we draw observations in accordance with
the relative frequency of the observations, will we obtain the same
result in some asymptotic sense? The answer is yes.</p>
<p>Instead of generating the histogram for the relative
frequency of the observation <span class="math notranslate nohighlight">\(X_i\)</span>, just draw the values
<span class="math notranslate nohighlight">\((X_1^*,X_2^*,\cdots,X_n^*)\)</span> with replacement from the vector
<span class="math notranslate nohighlight">\(\hat{X}\)</span>.</p>
</section>
<section id="resampling-methods-bootstrap-steps">
<h2><span class="section-number">12.21. </span>Resampling methods: Bootstrap steps<a class="headerlink" href="#resampling-methods-bootstrap-steps" title="Permalink to this heading">#</a></h2>
<p>The independent bootstrap works like this:</p>
<ol class="arabic simple">
<li><p>Draw with replacement <span class="math notranslate nohighlight">\(n\)</span> numbers for the observed variables <span class="math notranslate nohighlight">\(\hat{x} = (x_1,x_2,\cdots,x_n)\)</span>.</p></li>
<li><p>Define a vector <span class="math notranslate nohighlight">\(\hat{x}^*\)</span> containing the values which were drawn from <span class="math notranslate nohighlight">\(\hat{x}\)</span>.</p></li>
<li><p>Using the vector <span class="math notranslate nohighlight">\(\hat{x}^*\)</span> compute <span class="math notranslate nohighlight">\(\widehat{\theta}^*\)</span> by evaluating <span class="math notranslate nohighlight">\(\widehat \theta\)</span> under the observations <span class="math notranslate nohighlight">\(\hat{x}^*\)</span>.</p></li>
<li><p>Repeat this process <span class="math notranslate nohighlight">\(k\)</span> times.</p></li>
</ol>
<p>When you are done, you can draw a histogram of the relative frequency of <span class="math notranslate nohighlight">\(\widehat \theta^*\)</span>. This is your estimate of the probability distribution <span class="math notranslate nohighlight">\(p(t)\)</span>. Using this probability distribution you can estimate any statistics thereof. In principle you never draw the histogram of the relative frequency of <span class="math notranslate nohighlight">\(\widehat{\theta}^*\)</span>. Instead you use the estimators corresponding to the statistic of interest. For example, if you are interested in estimating the variance of <span class="math notranslate nohighlight">\(\widehat \theta\)</span>, apply the etsimator <span class="math notranslate nohighlight">\(\widehat \sigma^2\)</span> to the values <span class="math notranslate nohighlight">\(\widehat \theta ^*\)</span>.</p>
</section>
<section id="code-example-for-the-bootstrap-method">
<h2><span class="section-number">12.22. </span>Code example for the Bootstrap method<a class="headerlink" href="#code-example-for-the-bootstrap-method" title="Permalink to this heading">#</a></h2>
<p>The following code starts with a Gaussian distribution with mean value <span class="math notranslate nohighlight">\(\mu =100\)</span> and variance <span class="math notranslate nohighlight">\(\sigma=15\)</span>. We use this to generate the data used in the bootstrap analysis. The bootstrap analysis returns a data set after a given number of bootstrap operations (as many as we have data points). This data set consists of estimated mean values for each bootstrap operation. The histogram generated by the bootstrap method shows that the distribution for these mean values is also a Gaussian, centered around the mean value <span class="math notranslate nohighlight">\(\mu=100\)</span> but with standard deviation <span class="math notranslate nohighlight">\(\sigma/\sqrt{n}\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of bootstrap samples (in this case the same as the number of original data points). The value of the standard deviation is what we expect from the central limit theorem.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline


<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">randint</span><span class="p">,</span> <span class="n">randn</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Returns mean of bootstrap samples                                                                                                                                                </span>
<span class="k">def</span> <span class="nf">stat</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Bootstrap algorithm                                                                                                                                                              </span>
<span class="k">def</span> <span class="nf">bootstrap</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">statistic</span><span class="p">,</span> <span class="n">R</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">R</span><span class="p">);</span> <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">);</span> <span class="n">inds</span> <span class="o">=</span> <span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">);</span> <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>

    <span class="c1"># non-parametric bootstrap                                                                                                                                                     </span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">R</span><span class="p">):</span>
        <span class="n">t</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">statistic</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">)])</span>

    <span class="c1"># analysis                                                                                                                                                                     </span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Runtime: </span><span class="si">%g</span><span class="s2"> sec&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t0</span><span class="p">));</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bootstrap Statistics :&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;original           bias      std. error&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%8g</span><span class="s2"> </span><span class="si">%8g</span><span class="s2"> </span><span class="si">%14g</span><span class="s2"> </span><span class="si">%15g</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">statistic</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">std</span><span class="p">(</span><span class="n">data</span><span class="p">),</span>\
                             <span class="n">mean</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> \
                             <span class="n">std</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">t</span>


<span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">15</span>
<span class="n">datapoints</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">datapoints</span><span class="p">)</span>
<span class="c1"># bootstrap returns the data sample                                                                                                          t = bootstrap(x, stat, datapoints)</span>
<span class="c1"># the histogram of the bootstrapped  data  </span>
<span class="n">t</span> <span class="o">=</span> <span class="n">bootstrap</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stat</span><span class="p">,</span> <span class="n">datapoints</span><span class="p">)</span>
<span class="c1"># the histogram of the bootstrapped  data                                            </span>
<span class="n">n</span><span class="p">,</span> <span class="n">binsboot</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="s1">&#39;true&#39;</span><span class="p">,</span><span class="n">histtype</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>

<span class="c1"># add a &#39;best fit&#39; line                                                                                                                                                          </span>
<span class="n">y</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span> <span class="n">binsboot</span><span class="p">,</span> <span class="n">mean</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">std</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
<span class="n">lt</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">binsboot</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Smarts&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mf">99.5</span><span class="p">,</span> <span class="mf">100.6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">&#39;matplotlib&#39;</span><span class="p">,</span> <span class="s1">&#39;inline&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">&#39;matplotlib&#39;</span><span class="p">,</span> <span class="s1">&#39;inline&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="o">*</span>

<span class="nn">File ~/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2432,</span> in <span class="ni">InteractiveShell.run_line_magic</span><span class="nt">(self, magic_name, line, _stack_depth)</span>
<span class="g g-Whitespace">   </span><span class="mi">2430</span>     <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;local_ns&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_local_scope</span><span class="p">(</span><span class="n">stack_depth</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2431</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">builtin_trap</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">2432</span>     <span class="n">result</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2434</span> <span class="c1"># The code below prevents the output from being displayed</span>
<span class="g g-Whitespace">   </span><span class="mi">2435</span> <span class="c1"># when using magics with decorator @output_can_be_silenced</span>
<span class="g g-Whitespace">   </span><span class="mi">2436</span> <span class="c1"># when the last Python token in the expression is a &#39;;&#39;.</span>
<span class="g g-Whitespace">   </span><span class="mi">2437</span> <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">magic</span><span class="o">.</span><span class="n">MAGIC_OUTPUT_CAN_BE_SILENCED</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>

<span class="nn">File ~/miniforge3/lib/python3.9/site-packages/IPython/core/magics/pylab.py:99,</span> in <span class="ni">PylabMagics.matplotlib</span><span class="nt">(self, line)</span>
<span class="g g-Whitespace">     </span><span class="mi">97</span>     <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Available matplotlib backends: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">backends_list</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">98</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">99</span>     <span class="n">gui</span><span class="p">,</span> <span class="n">backend</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shell</span><span class="o">.</span><span class="n">enable_matplotlib</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gui</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gui</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">gui</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">100</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_show_matplotlib_backend</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gui</span><span class="p">,</span> <span class="n">backend</span><span class="p">)</span>

<span class="nn">File ~/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3606,</span> in <span class="ni">InteractiveShell.enable_matplotlib</span><span class="nt">(self, gui)</span>
<span class="g g-Whitespace">   </span><span class="mi">3585</span> <span class="k">def</span> <span class="nf">enable_matplotlib</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gui</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">3586</span><span class="w">     </span><span class="sd">&quot;&quot;&quot;Enable interactive matplotlib and inline figure support.</span>
<span class="g g-Whitespace">   </span><span class="mi">3587</span><span class="sd"> </span>
<span class="g g-Whitespace">   </span><span class="mi">3588</span><span class="sd">     This takes the following steps:</span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">   </span><span class="mi">3604</span><span class="sd">         display figures inline.</span>
<span class="g g-Whitespace">   </span><span class="mi">3605</span><span class="sd">     &quot;&quot;&quot;</span>
<span class="ne">-&gt; </span><span class="mi">3606</span>     <span class="kn">from</span> <span class="nn">matplotlib_inline.backend_inline</span> <span class="kn">import</span> <span class="n">configure_inline_support</span>
<span class="g g-Whitespace">   </span><span class="mi">3608</span>     <span class="kn">from</span> <span class="nn">IPython.core</span> <span class="kn">import</span> <span class="n">pylabtools</span> <span class="k">as</span> <span class="n">pt</span>
<span class="g g-Whitespace">   </span><span class="mi">3609</span>     <span class="n">gui</span><span class="p">,</span> <span class="n">backend</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">find_gui_and_backend</span><span class="p">(</span><span class="n">gui</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pylab_gui_select</span><span class="p">)</span>

<span class="n">File</span> <span class="o">~/</span><span class="n">miniforge3</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.9</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">matplotlib_inline</span><span class="o">/</span><span class="fm">__init__</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">backend_inline</span><span class="p">,</span> <span class="n">config</span>  <span class="c1"># noqa</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">__version__</span> <span class="o">=</span> <span class="s2">&quot;0.1.6&quot;</span>  <span class="c1"># noqa</span>

<span class="n">File</span> <span class="o">~/</span><span class="n">miniforge3</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.9</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">matplotlib_inline</span><span class="o">/</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">6</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span><span class="w"> </span><span class="sd">&quot;&quot;&quot;A matplotlib backend for publishing figures via display_data&quot;&quot;&quot;</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="c1"># Copyright (c) IPython Development Team.</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="c1"># Distributed under the terms of the BSD 3-Clause License.</span>
<span class="ne">----&gt; </span><span class="mi">6</span> <span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">colors</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="kn">from</span> <span class="nn">matplotlib.backends</span> <span class="kn">import</span> <span class="n">backend_agg</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;matplotlib&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="resampling-methods-blocking">
<h2><span class="section-number">12.23. </span>Resampling methods: Blocking<a class="headerlink" href="#resampling-methods-blocking" title="Permalink to this heading">#</a></h2>
<p>The blocking method was made popular by <a class="reference external" href="https://aip.scitation.org/doi/10.1063/1.457480">Flyvbjerg and Pedersen (1989)</a>
and has become one of the standard ways to estimate
<span class="math notranslate nohighlight">\(V(\widehat{\theta})\)</span> for exactly one <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span>, namely
<span class="math notranslate nohighlight">\(\widehat{\theta} = \overline{X}\)</span>.</p>
<p>Assume <span class="math notranslate nohighlight">\(n = 2^d\)</span> for some integer <span class="math notranslate nohighlight">\(d&gt;1\)</span> and <span class="math notranslate nohighlight">\(X_1,X_2,\cdots, X_n\)</span> is a stationary time series to begin with.
Moreover, assume that the time series is asymptotically uncorrelated. We switch to vector notation by arranging <span class="math notranslate nohighlight">\(X_1,X_2,\cdots,X_n\)</span> in an <span class="math notranslate nohighlight">\(n\)</span>-tuple. Define:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\hat{X} = (X_1,X_2,\cdots,X_n).
\end{align*}
\]</div>
<p>The strength of the blocking method is when the number of
observations, <span class="math notranslate nohighlight">\(n\)</span> is large. For large <span class="math notranslate nohighlight">\(n\)</span>, the complexity of dependent
bootstrapping scales poorly, but the blocking method does not,
moreover, it becomes more accurate the larger <span class="math notranslate nohighlight">\(n\)</span> is.</p>
</section>
<section id="blocking-transformations">
<h2><span class="section-number">12.24. </span>Blocking Transformations<a class="headerlink" href="#blocking-transformations" title="Permalink to this heading">#</a></h2>
<p>We now define
blocking transformations. The idea is to take the mean of subsequent
pair of elements from <span class="math notranslate nohighlight">\(\vec{X}\)</span> and form a new vector
<span class="math notranslate nohighlight">\(\vec{X}_1\)</span>. Continuing in the same way by taking the mean of
subsequent pairs of elements of <span class="math notranslate nohighlight">\(\vec{X}_1\)</span> we obtain <span class="math notranslate nohighlight">\(\vec{X}_2\)</span>, and
so on.
Define <span class="math notranslate nohighlight">\(\vec{X}_i\)</span> recursively by:</p>
<div class="math notranslate nohighlight">
\[
(\vec{X}_0)_k \equiv (\vec{X})_k \nonumber
\]</div>
<!-- Equation labels as ordinary links -->
<div id="_auto2"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} 
(\vec{X}_{i+1})_k \equiv \frac{1}{2}\Big( (\vec{X}_i)_{2k-1} +
(\vec{X}_i)_{2k} \Big) \qquad \text{for all} \qquad 1 \leq i \leq d-1
\label{_auto2} \tag{5}
\end{equation}
\]</div>
<p>The quantity <span class="math notranslate nohighlight">\(\vec{X}_k\)</span> is
subject to <span class="math notranslate nohighlight">\(k\)</span> <strong>blocking transformations</strong>.  We now have <span class="math notranslate nohighlight">\(d\)</span> vectors
<span class="math notranslate nohighlight">\(\vec{X}_0, \vec{X}_1,\cdots,\vec X_{d-1}\)</span> containing the subsequent
averages of observations. It turns out that if the components of
<span class="math notranslate nohighlight">\(\vec{X}\)</span> is a stationary time series, then the components of
<span class="math notranslate nohighlight">\(\vec{X}_i\)</span> is a stationary time series for all <span class="math notranslate nohighlight">\(0 \leq i \leq d-1\)</span></p>
<p>We can then compute the autocovariance, the variance, sample mean, and
number of observations for each <span class="math notranslate nohighlight">\(i\)</span>.
Let <span class="math notranslate nohighlight">\(\gamma_i, \sigma_i^2,
\overline{X}_i\)</span> denote the autocovariance, variance and average of the
elements of <span class="math notranslate nohighlight">\(\vec{X}_i\)</span> and let <span class="math notranslate nohighlight">\(n_i\)</span> be the number of elements of
<span class="math notranslate nohighlight">\(\vec{X}_i\)</span>. It follows by induction that <span class="math notranslate nohighlight">\(n_i = n/2^i\)</span>.</p>
</section>
<section id="id5">
<h2><span class="section-number">12.25. </span>Blocking Transformations<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h2>
<p>Using the
definition of the blocking transformation and the distributive
property of the covariance, it is clear that since <span class="math notranslate nohighlight">\(h =|i-j|\)</span>
we can define</p>
<div class="math notranslate nohighlight">
\[
\gamma_{k+1}(h) = cov\left( ({X}_{k+1})_{i}, ({X}_{k+1})_{j} \right) \nonumber
\]</div>
<div class="math notranslate nohighlight">
\[
=  \frac{1}{4}cov\left( ({X}_{k})_{2i-1} + ({X}_{k})_{2i}, ({X}_{k})_{2j-1} + ({X}_{k})_{2j} \right) \nonumber
\]</div>
<!-- Equation labels as ordinary links -->
<div id="_auto3"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} 
=  \frac{1}{2}\gamma_{k}(2h) + \frac{1}{2}\gamma_k(2h+1) \hspace{0.1cm} \mathrm{h = 0} 
\label{_auto3} \tag{6}
\end{equation}
\]</div>
<!-- Equation labels as ordinary links -->
<div id="_auto4"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} 
=\frac{1}{4}\gamma_k(2h-1) + \frac{1}{2}\gamma_k(2h) + \frac{1}{4}\gamma_k(2h+1) \quad \mathrm{else}
\label{_auto4} \tag{7}
\end{equation}
\]</div>
<p>The quantity <span class="math notranslate nohighlight">\(\hat{X}\)</span> is asymptotic uncorrelated by assumption, <span class="math notranslate nohighlight">\(\hat{X}_k\)</span> is also asymptotic uncorrelated. Let’s turn our attention to the variance of the sample mean <span class="math notranslate nohighlight">\(V(\overline{X})\)</span>.</p>
</section>
<section id="blocking-transformations-getting-there">
<h2><span class="section-number">12.26. </span>Blocking Transformations, getting there<a class="headerlink" href="#blocking-transformations-getting-there" title="Permalink to this heading">#</a></h2>
<p>We have</p>
<!-- Equation labels as ordinary links -->
<div id="_auto5"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
V(\overline{X}_k) = \frac{\sigma_k^2}{n_k} + \underbrace{\frac{2}{n_k} \sum_{h=1}^{n_k-1}\left( 1 - \frac{h}{n_k} \right)\gamma_k(h)}_{\equiv e_k} = \frac{\sigma^2_k}{n_k} + e_k \quad \text{if} \quad \gamma_k(0) = \sigma_k^2. 
\label{_auto5} \tag{8}
\end{equation}
\]</div>
<p>The term <span class="math notranslate nohighlight">\(e_k\)</span> is called the <strong>truncation error</strong>:</p>
<!-- Equation labels as ordinary links -->
<div id="_auto6"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
e_k = \frac{2}{n_k} \sum_{h=1}^{n_k-1}\left( 1 - \frac{h}{n_k} \right)\gamma_k(h). 
\label{_auto6} \tag{9}
\end{equation}
\]</div>
<p>We can show that <span class="math notranslate nohighlight">\(V(\overline{X}_i) = V(\overline{X}_j)\)</span> for all <span class="math notranslate nohighlight">\(0 \leq i \leq d-1\)</span> and <span class="math notranslate nohighlight">\(0 \leq j \leq d-1\)</span>.</p>
</section>
<section id="blocking-transformations-final-expressions">
<h2><span class="section-number">12.27. </span>Blocking Transformations, final expressions<a class="headerlink" href="#blocking-transformations-final-expressions" title="Permalink to this heading">#</a></h2>
<p>We can then wrap up</p>
<div class="math notranslate nohighlight">
\[
n_{j+1} \overline{X}_{j+1}  = \sum_{i=1}^{n_{j+1}} (\hat{X}_{j+1})_i =  \frac{1}{2}\sum_{i=1}^{n_{j}/2} (\hat{X}_{j})_{2i-1} + (\hat{X}_{j})_{2i} \nonumber
\]</div>
<!-- Equation labels as ordinary links -->
<div id="_auto7"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} 
= \frac{1}{2}\left[ (\hat{X}_j)_1 + (\hat{X}_j)_2 + \cdots + (\hat{X}_j)_{n_j} \right] = \underbrace{\frac{n_j}{2}}_{=n_{j+1}} \overline{X}_j = n_{j+1}\overline{X}_j. 
\label{_auto7} \tag{10}
\end{equation}
\]</div>
<p>By repeated use of this equation we get <span class="math notranslate nohighlight">\(V(\overline{X}_i) = V(\overline{X}_0) = V(\overline{X})\)</span> for all <span class="math notranslate nohighlight">\(0 \leq i \leq d-1\)</span>. This has the consequence that</p>
<!-- Equation labels as ordinary links -->
<div id="eq:convergence"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
V(\overline{X}) = \frac{\sigma_k^2}{n_k} + e_k \qquad \text{for all} \qquad 0 \leq k \leq d-1. \label{eq:convergence} \tag{11}
\end{equation}
\]</div>
<p>Flyvbjerg and Petersen demonstrated that the sequence
<span class="math notranslate nohighlight">\(\{e_k\}_{k=0}^{d-1}\)</span> is decreasing, and conjecture that the term
<span class="math notranslate nohighlight">\(e_k\)</span> can be made as small as we would like by making <span class="math notranslate nohighlight">\(k\)</span> (and hence
<span class="math notranslate nohighlight">\(d\)</span>) sufficiently large. The sequence is decreasing (Master of Science thesis by Marius Jonsson, UiO 2018).
It means we can apply blocking transformations until
<span class="math notranslate nohighlight">\(e_k\)</span> is sufficiently small, and then estimate <span class="math notranslate nohighlight">\(V(\overline{X})\)</span> by
<span class="math notranslate nohighlight">\(\widehat{\sigma}^2_k/n_k\)</span>.</p>
<p>For an elegant solution and proof of the blocking method, see the recent article of <a class="reference external" href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.98.043304">Marius Jonsson (former MSc student of the Computational Physics group)</a>.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="gradientmethods.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Gradient Methods</p>
      </div>
    </a>
    <a class="right-next"
       href="boltzmannmachines.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">13. </span>Boltzmann Machines</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-resampling-methods">12.1. Why resampling methods ?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-analysis">12.2. Statistical analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics-wrapping-up-from-last-week">12.3. Statistics, wrapping up from last week</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics-final-expression">12.4. Statistics, final expression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics-effective-number-of-correlations">12.5. Statistics, effective number of correlations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#can-we-understand-this-time-auto-correlation-function">12.6. Can we understand this? Time Auto-correlation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-auto-correlation-function">12.7. Time Auto-correlation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">12.8. Time Auto-correlation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">12.9. Time Auto-correlation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">12.10. Time Auto-correlation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">12.11. Time Auto-correlation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-time">12.12. Correlation Time</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-jackknife-and-bootstrap">12.13. Resampling methods: Jackknife and Bootstrap</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-jackknife">12.14. Resampling methods: Jackknife</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-jackknife-estimator">12.15. Resampling methods: Jackknife estimator</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jackknife-code-example">12.16. Jackknife code example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-bootstrap">12.17. Resampling methods: Bootstrap</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-bootstrap-background">12.18. Resampling methods: Bootstrap background</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-more-bootstrap-background">12.19. Resampling methods: More Bootstrap background</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-bootstrap-approach">12.20. Resampling methods: Bootstrap approach</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-bootstrap-steps">12.21. Resampling methods: Bootstrap steps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-example-for-the-bootstrap-method">12.22. Code example for the Bootstrap method</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling-methods-blocking">12.23. Resampling methods: Blocking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#blocking-transformations">12.24. Blocking Transformations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">12.25. Blocking Transformations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#blocking-transformations-getting-there">12.26. Blocking Transformations, getting there</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#blocking-transformations-final-expressions">12.27. Blocking Transformations, final expressions</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Morten Hjorth-Jensen
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
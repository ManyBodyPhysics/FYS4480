<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week34-reveal.html week34-reveal reveal --html_slide_theme=beige
-->
<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Many-body Physics; Introduction to the course, notations and definitions">
<title>Many-body Physics; Introduction to the course, notations and definitions</title>

<!-- reveal.js: https://lab.hakim.se/reveal-js/ -->

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<!--
<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/beigesmall.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/serif.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/moon.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/sky.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/darkgray.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/cbc.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simula.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/league.css" id="theme">
-->

<!-- For syntax highlighting -->
<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>

<style type="text/css">
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
.reveal .alert-text-small   { font-size: 80%;  }
.reveal .alert-text-large   { font-size: 130%; }
.reveal .alert-text-normal  { font-size: 90%;  }
.reveal .alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:5px solid #bababa;
  -webkit-border-radius: 14px; -moz-border-radius: 14px;
  border-radius:14px;
  background-position: 10px 10px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 30px; /* 55px; if icon */
}
.reveal .alert-block {padding-top:14px; padding-bottom:14px}
.reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
/*.reveal .alert li {margin-top: 1em}*/
.reveal .alert-block p+p {margin-top:5px}
/*.reveal .alert-notice { background-image: url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_notice.png); }
.reveal .alert-summary  { background-image:url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_summary.png); }
.reveal .alert-warning { background-image: url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_warning.png); }
.reveal .alert-question {background-image:url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_question.png); } */
/* Override reveal.js table border */
.reveal table td {
  border: 0;
}

<style type="text/css">
/* Override h1, h2, ... styles */
h1 { font-size: 2.8em; }
h2 { font-size: 1.5em; }
h3 { font-size: 1.4em; }
h4 { font-size: 1.3em; }
h1, h2, h3, h4 { font-weight: bold; line-height: 1.2; }
body { overflow: auto; } /* vertical scrolling */
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
.slide .alert-text-small   { font-size: 80%;  }
.slide .alert-text-large   { font-size: 130%; }
.slide .alert-text-normal  { font-size: 90%;  }
.slide .alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:5px solid #bababa;
    -webkit-border-radius:14px; -moz-border-radius:14px;
  border-radius:14px
  background-position: 10px 10px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 30px; /* 55px; if icon */
}
.slide .alert-block {padding-top:14px; padding-bottom:14px}
.slide .alert-block > p, .alert-block > ul {margin-bottom:0}
/*.slide .alert li {margin-top: 1em}*/
.deck .alert-block p+p {margin-top:5px}
/*.slide .alert-notice { background-image: url(https://hplgit.github.io/doconce/
bundled/html_images//small_gray_notice.png); }
.slide .alert-summary  { background-image:url(https://hplgit.github.io/doconce/
bundled/html_images//small_gray_summary.png); }
.slide .alert-warning { background-image: url(https://hplgit.github.io/doconce/
bundled/html_images//small_gray_warning.png); }
.slide .alert-question {background-image:url(https://hplgit.github.io/doconce/
bundled/html_images/small_gray_question.png); } */
.dotable table, .dotable th, .dotable tr, .dotable tr td {
  border: 2px solid black;
  border-collapse: collapse;
  padding: 2px;
}
</style>


<!-- Styles for table layout of slides -->
<style type="text/css">
td.padding {
  padding-top:20px;
  padding-bottom:20px;
  padding-right:50px;
  padding-left:50px;
}
</style>

</head>


<body>
<div class="reveal">
<div class="slides">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




<section>
<!-- ------------------- main content ---------------------- -->
<center>
<h1 style="text-align: center;">Many-body Physics; Introduction to the course, notations and definitions</h1>
</center>  <!-- document title -->

<!-- author(s): Morten Hjorth-Jensen -->
<center>
<b>Morten Hjorth-Jensen</b> 
</center>
<!-- institution -->
<center>
<b>Department of Physics and Center for Computing in Science Education, University of Oslo, Norway</b>
</center>
<br>
<center>
<h4>Week 34, August 19-23, 2024</h4>
</center> <!-- date -->
<br>


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2024, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</section>

<section>
<h2 id="week-34">Week 34  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ul>
<p><li> Topics to be covered
<ol type="a"></li>
 <p><li> Thursday: Introduction to many-body physics, notations, definitions and plans for the semester</li>
<ul>

<p><li> Linear algebra reminder and basic mathematical background</li>

<p><li> Hamiltonians and basis sets</li>
</ul>
<p>
 <p><li> Video of lecture to be added (TBA) at <a href="https://youtu.be/" target="_blank"><tt>https://youtu.be/</tt></a></li>
 <p><li> Friday: Fermions and bosons, basis sets and quantum mechanical expectation values</li>
<ul>

<p><li> Exercise session, see exercise set for week 34 at <a href="https://github.com/ManyBodyPhysics/FYS4480/tree/master/doc/Exercises/2024" target="_blank"><tt>https://github.com/ManyBodyPhysics/FYS4480/tree/master/doc/Exercises/2024</tt></a></li>
</ul>
<p>
 <p><li> Video of lecture TBA at <a href="https://youtu.be" target="_blank"><tt>https://youtu.be</tt></a></li>
</ol>
<p>
<p><li> Lecture Material: These slides and Sazbo and Ostlund chapters 1 and 2.</li>
</ul>
</div>
</section>

<section>
<h2 id="introduction-to-the-course">Introduction to the course </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>These lecture aim at giving an introduction to the quantum mechanics
of many-body systems and the methods relevant for many-body problems
in such diverse areas as atomic, molecular, solid-state and nuclear
physics, chemistry and materials science. A theoretical understanding
of the behavior of quantum-mechanical many-body systems, that is,
systems containing many interacting particles, is a considerable
challenge in that, normally, no exact solution can be found.  Instead,
reliable methods are needed for approximate but accurate simulations
of such systems.
</p>
</div>
</section>

<section>
<h2 id="content-for-many-body-lecture-notes-intro">Content for many-body lecture notes, intro </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ul>
<p><li> Intro chapter with basic definitions and simple examples and mathematics of many-body functions</li>
<ul>

<p><li> Definitions of SDs etc, permutation operators,linear algebra reminder including reminder about determinants, vector and mtx algebra, tensor products, representations, unitary transformations, link to quantities like</li>

<p><li> one-body and two-body densities, rms radii etc. Discuss ansatze for wave functions and more.</li>

<p><li> Ansaztes for wave functions</li>
</ul>
<p>
</ul>
</div>
</section>

<section>
<h2 id="content-for-many-body-lecture-notes-2nd-quantization">Content for many-body lecture notes, 2nd quantization  </h2>

<ul>
<p><li> 2nd quantization for bosons and fermions and more</li>
<ul>

<p><li> Commutation rules and definition of creation and annihilation operators</li>

<p><li> Proof of wick's theorem</li>

<p><li> Discuss Wick's generalized theorem</li>

<p><li> particle-hole picture</li>

<p><li> interaction, Schroedinger and Heisenberg pictures, pros and cons</li>

<p><li> time dependent wick's theorem</li>

<p><li> Gell-Man and Low's theorem</li>

<p><li> Adiabatic switching</li>

<p><li> Derivation of expressions for different parts of Hamiltonians, 1b, 2b, 3b etc</li>

<p><li> Wigner-Jordan transformation and 2nd quantization</li>

<p><li> Baker-Campbell-Hausdorf (BCH)</li>

<p><li> Suzuki-Trotter as an approximation to BCH</li>
</ul>
<p>
</ul>
</section>

<section>
<h2 id="content-for-many-body-lecture-notes-fci">Content for many-body lecture notes, FCI </h2>

<ul>
<p><li> FCI and diagrams and particle-hole representations</li>
<ul>

<p><li> Basics of FCI</li>

<p><li> Rewriting in terms of a particle-hole picture</li>

<p><li> Discuss slater determinants and similarity transformations and algorithms for solving eigenvalue problems</li>

<p><li> Eigenvector continuation</li>

<p><li> Introduce a diagrammatic representation</li>
</ul>
<p>
</ul>
</section>

<section>
<h2 id="content-for-many-body-lecture-notes-mean-filed-theories">Content for many-body lecture notes, Mean-filed theories </h2>

<ul>
<p><li> Mean-field theories</li>
<ul>

<p><li> Hartree-Fock in coordinate space and 2nd quantization</li>

<p><li> Thouless theorem</li>

<p><li> Slater dets in HF theory</li>

<p><li> DFT links</li>

<p><li> The electron gas as example</li>

<p><li> FCI and HF, diagrammatic representations and critical discussions</li>
</ul>
<p>
</ul>
</section>

<section>
<h2 id="content-for-many-body-lecture-notes-perturbation-theory">Content for many-body lecture notes, perturbation theory </h2>

<ul>
<p><li> Many-body perturbation theory</li>
<ul>

<p><li> Time dependent and time-independent representation</li>

<p><li> Brillouin-Wigner and Rayleigh-Schr&#248;dinger pert theory</li>

<p><li> Diagrammatic representation</li>

<p><li> Linked-diagram theorem based on time-dependent theory</li>
</ul>
<p>
</ul>
</section>

<section>
<h2 id="content-for-many-body-lecture-notes-coupled-cluster-theory">Content for many-body lecture notes, Coupled Cluster theory </h2>
<ul>
<p><li> Coupled cluster theories, standard and unitary</li>
<ul>

<p><li> Derivation of equations for singles and doubles, reminder on unitary transformations</li>

<p><li> non-hermiticity</li>

<p><li> Specialize to CCD case and compare with FCI and MBPT</li>
</ul>
<p>
</ul>
</section>

<section>
<h2 id="content-for-many-body-lecture-notes-green-s-function-theory-and-other-methods">Content for many-body lecture notes, Green's function theory and other methods  </h2>
<ol>
<p><li> Green's function theory and parquet theory</li>
<p><li> Monte Carlo methods</li>
<ul>

<p><li> Covered by FYS4411</li>
</ul>
<p>
<p><li> Quantum computing</li>
<ul>

<p><li> VQE and unitary CC, also covered by FYS5419</li>
</ul>
<p>
<p><li> Time-dependent many-body theory</li>
<p><li> Applications to different systems like the electron gas, Lipkin model, Pairing model, infinite nuclear matter, and more</li>
</ol>
</section>

<section>
<h2 id="recommended-textbooks">Recommended textbooks </h2>

<p>In the folder <a href="https://github.com/ManyBodyPhysics/FYS4480/tree/master/doc/Literature" target="_blank"><tt>https://github.com/ManyBodyPhysics/FYS4480/tree/master/doc/Literature</tt></a> you will find the textbooks we will be following.
Weekly reading assignments based on these texts will be sent before each week.
In particular we recommend the texts by
</p>
<ol>
<p><li> <a href="https://www.amazon.com/Modern-Quantum-Chemistry-Introduction-Electronic/dp/0486691861?asin=0486691861&revisionId=&format=4&depth=1" target="_blank">Szabo and Ostlund</a></li>
<p><li> <a href="https://www.cambridge.org/core/books/manybody-methods-in-chemistry-and-physics/D12027E4DAF75CE8214671D842C6B80C" target="_blank">Shavitt and Bartlett</a></li>
</ol>
</section>

<section>
<h2 id="teaching-mode">Teaching mode </h2>

<p>This course will be delivered in a hybrid mode, with online and on
site lectures and on site exercise sessions. Only lectures are
recorded.
</p>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<p><li> Four  lectures per week, fall semester, 10 ECTS. The lectures will be recorded and linked to this site and the official University of Oslo website for the course;</li>
<p><li> Two hours of exercise sessions for work on projects and exercises;</li>
<p><li> Two projects which are graded and count 30%  each of the final grade;</li>
<p><li> A final oral exam which counts 40% of the final grade</li>
<p><li> The course is offered as  FYS4480 (Master of Science level) and as FYS9480 (PhD level);
<ol type="a"></li>
<p><li> Videos of teaching material are available via the links at <a href="https://www.uio.no/studier/emner/matnat/fys/FYS4480/h23/timeplan/index.html" target="_blank"><tt>https://www.uio.no/studier/emner/matnat/fys/FYS4480/h23/timeplan/index.html</tt></a></li>
</ol>
<p>
</ol>
<p>
<p>6. Weekly emails with summary of activities will be mailed to all participants;</p>
</div>
</section>

<section>
<h2 id="notations-and-definitions">Notations and definitions  </h2>

<p>Vectors, matrices and higher-order tensors are always boldfaced, with vectors
given by lower case letter letters and matrices and higher-order tensors given by upper case letters.
</p>

<p>Unless otherwise stated, the elements \( v_i \) of a vector \( \boldsymbol{v} \) are assumed to be real. That is a vector of length \( n \) is defined as
\( \boldsymbol{x}\in \mathbb{R}^{n} \) and if we have a complex vector we have \( \boldsymbol{x}\in \mathbb{C}^{n} \).
</p>

<p>For a matrix of dimension \( n\times n \) we have 
\( \boldsymbol{A}\in \mathbb{R}^{n\times n} \) and the first matrix element starts with row element (row-wise ordering) zero and column element zero.
</p>
</section>

<section>
<h2 id="some-mathematical-notations">Some  mathematical notations </h2>
<ol>
<p><li> For all/any  \( \forall \)</li>
<p><li> Implies \( \implies \)</li>
<p><li> Equivalent \( \equiv \)</li>
<p><li> Real variable \( \mathbb{R} \)</li>
<p><li> Integer variable \( \mathbb{I} \)</li>
<p><li> Complex  variable \( \mathbb{C} \)</li>
</ol>
</section>

<section>
<h2 id="vectors">Vectors </h2>

<p>We start by defining a vector \( \boldsymbol{x} \)  with \( n \) components, with \( x_0 \) as our first element, as</p>

<p>&nbsp;<br>
$$
\boldsymbol{x} = \begin{bmatrix} x_0\\ x_1 \\ x_2 \\ \dots \\ \dots \\ x_{n-1} \end{bmatrix}.
$$
<p>&nbsp;<br>

<p>and its transpose </p>
<p>&nbsp;<br>
$$
\boldsymbol{x}^{T} = \begin{bmatrix} x_0 & x_1 & x_2 & \dots & \dots & x_{n-1} \end{bmatrix},
$$
<p>&nbsp;<br>

<p>In case we have a complex vector we define the hermitian conjugate</p>
<p>&nbsp;<br>
$$
\boldsymbol{x}^{\dagger} = \begin{bmatrix} x_0^* & x_1^* & x_2^* & \dots & \dots & x_{n-1}^* \end{bmatrix},
$$
<p>&nbsp;<br>

<p>With a given vector \( \boldsymbol{x} \), we define the inner product as</p>
<p>&nbsp;<br>
$$
\boldsymbol{x}^T \boldsymbol{x} = \sum_{i=0}^{n-1} x_ix_i=x_0^2+x_1^2+\dots + x_{n-1}^2. 
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="hermitian-conjugate">Hermitian conjugate </h2>

<p>The hermitian conjugate of a matrix is obtained by taking the complex
conjugate of each element and then taking the transpose of the
resulting matrix. Often we will just say the transpose or just the
conjugate, it should be clear from the context that we will mainly
deal with hermitian quantities and our matrices will in most cases be square matrices.
</p>

<p>Unitarity, as we will see below, plays also a central role in this course.</p>
</section>

<section>
<h2 id="outer-products">Outer products </h2>

<p>In addition to inner products between vectors/states, the outer
product plays a central role in many applications. It is
defined as
</p>
<p>&nbsp;<br>
$$
\boldsymbol{x}\boldsymbol{y}^T = \begin{bmatrix}
               x_0y_0 & x_0y_1 & x_0y_2 & \dots & \dots & x_0y_{n-2} & x_0y_{n-1} \\
	       x_1y_0 & x_1y_1 & x_1y_2 & \dots & \dots & x_1y_{n-2} & x_1y_{n-1} \\
	       x_2y_0 & x_2y_1 & x_2y_2 & \dots & \dots & x_2y_{n-2} & x_2y_{n-1} \\	       
               \dots &   \dots   & \dots  & \dots & \dots & \dots & \dots \\
               \dots &   \dots   & \dots  & \dots & \dots & \dots & \dots \\	       
	       x_{n-2}y_0 & x_{n-2}y_1 & x_{n-2}y_2 & \dots & \dots & x_{n-2}y_{n-2} & x_{n-2}y_{n-1} \\
	       x_{n-1}y_0 & x_{n-1}y_1 & x_{n-1}y_2 & \dots & \dots & x_{n-1}y_{n-2} & x_{n-1}y_{n-1} \end{bmatrix}	       
$$
<p>&nbsp;<br>

<p>The latter defines also our basic matrix layout.</p>
</section>

<section>
<h2 id="basic-matrix-features">Basic Matrix Features </h2>

<p>A general \( n\times n \) matrix is given by </p>
<p>&nbsp;<br>
$$
 \boldsymbol{A} =
\begin{bmatrix}
               a_{00} & a_{01} & a_{02} & \dots & \dots & a_{0n-2} & a_{0n-1} \\
               a_{10} & a_{11} & a_{12} & \dots & \dots & a_{1n-2} & a_{1n-1} \\
               \dots &   \dots   & \dots  & \dots & \dots & \dots & \dots \\
               \dots &   \dots   & \dots  & \dots & \dots & \dots & \dots \\	       
               a_{n-20} & a_{n-21} & a_{n-22} & \dots & \dots & a_{n-2n-2} & a_{n-2n-1} \\
               a_{n-10} & a_{n-11} & a_{n-12} & \dots & \dots & a_{n-1n-2} & a_{n-1n-1} \end{bmatrix},	       
$$
<p>&nbsp;<br>

<p>or in terms of its column vectors \( \boldsymbol{a}_i \) as</p>
<p>&nbsp;<br>
$$
 \boldsymbol{A} =
\begin{bmatrix}\boldsymbol{a}_{0} & \boldsymbol{a}_{1} & \boldsymbol{a}_{2} & \dots & \dots & \boldsymbol{a}_{n-2} & \boldsymbol{a}_{n-1}\end{bmatrix}.	       
$$
<p>&nbsp;<br>

<p>We can think of a matrix as a diagram of in general \( n \) rowns and \( m \) columns. In the example here we have a square matrix.</p>
</section>

<section>
<h2 id="the-inverse-of-a-matrix">The inverse of a matrix  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>The inverse of a square matrix (if it exists) is defined by</p>

<p>&nbsp;<br>
$$
\boldsymbol{A}^{-1} \cdot \boldsymbol{A} = I,
$$
<p>&nbsp;<br>

<p>where \( \boldsymbol{I} \) is the unit matrix.</p>
</div>
</section>

<section>
<h2 id="selected-matrix-features">Selected Matrix Features </h2>

<table class="dotable" border="1">
<thead>
<tr><th align="center">                             Relations                              </th> <th align="center">      Name     </th> <th align="center">                              matrix elements                              </th> </tr>
</thead>
<tbody>
<tr><td align="center">   \( \boldsymbol{A} = \boldsymbol{A}^{T} \)                               </td> <td align="center">   symmetric          </td> <td align="center">   \( a_{ij} = a_{ji} \)                                                          </td> </tr>
<tr><td align="center">   \( \boldsymbol{A} = \left (\boldsymbol{A}^{T} \right )^{-1} \)          </td> <td align="center">   real orthogonal    </td> <td align="center">   \( \sum_k a_{ik} a_{jk} = \sum_k a_{ki} a_{kj} = \delta_{ij} \)                </td> </tr>
<tr><td align="center">   \( \boldsymbol{A} = \boldsymbol{A}^{ * } \)                             </td> <td align="center">   real matrix        </td> <td align="center">   \( a_{ij} = a_{ij}^{ * } \)                                                    </td> </tr>
<tr><td align="center">   \( \boldsymbol{A} = \boldsymbol{A}^{\dagger} \)                         </td> <td align="center">   hermitian          </td> <td align="center">   \( a_{ij} = a_{ji}^{ * } \)                                                    </td> </tr>
<tr><td align="center">   \( \boldsymbol{A} = \left (\boldsymbol{A}^{\dagger} \right )^{-1} \)    </td> <td align="center">   unitary            </td> <td align="center">   \( \sum_k a_{ik} a_{jk}^{ * } = \sum_k a_{ki}^{ * } a_{kj} = \delta_{ij} \)    </td> </tr>
</tbody>
</table>
</section>

<section>
<h2 id="some-famous-matrices">Some famous Matrices </h2>

<ul>

<p><li> Diagonal if \( a_{ij}=0 \) for \( i\ne j \)</li>

<p><li> Upper triangular if \( a_{ij}=0 \) for \( i > j \)</li>

<p><li> Lower triangular if \( a_{ij}=0 \) for \( i < j \)</li>

<p><li> Upper Hessenberg if \( a_{ij}=0 \) for \( i > j+1 \)</li>

<p><li> Lower Hessenberg if \( a_{ij}=0 \) for \( i < j+1 \)</li>

<p><li> Tridiagonal if \( a_{ij}=0 \) for \( |i -j| > 1 \)</li>

<p><li> Lower banded with bandwidth \( p \): \( a_{ij}=0 \) for \( i > j+p \)</li>

<p><li> Upper banded with bandwidth \( p \): \( a_{ij}=0 \) for \( i < j+p \)</li>

<p><li> Banded, block upper triangular, block lower triangular....</li>
</ul>
</section>

<section>
<h2 id="matrix-features">Matrix Features </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Some equivalent statements for square matrices</b>
<p>
<p>For an \( n\times n \) matrix  \( \boldsymbol{A} \) the following properties are all equivalent</p>

<ul>

<p><li> If the inverse of \( \boldsymbol{A} \) exists, \( \boldsymbol{A} \) is nonsingular.</li>

<p><li> The equation \( \boldsymbol{Ax}=0 \) implies \( \boldsymbol{x}=0 \).</li>

<p><li> The rows of \( \boldsymbol{A} \) form a basis of \( \mathbb{C}^{n} \).</li>

<p><li> The columns of \( \boldsymbol{A} \) form a basis of \( \mathbb{C}^{n} \).</li>

<p><li> \( \boldsymbol{A} \) is a product of elementary matrices. Can you name one example?</li>

<p><li> \( 0 \) is not an eigenvalue of \( \boldsymbol{A} \).</li>
</ul>
</div>
</section>

<section>
<h2 id="important-mathematical-operations">Important Mathematical Operations </h2>

<p>The basic matrix operations that we will deal with are addition and subtraction</p>

<p>&nbsp;<br>
$$
\boldsymbol{A}= \boldsymbol{B}\pm\boldsymbol{C}  \Longrightarrow a_{ij} = b_{ij}\pm c_{ij},
$$
<p>&nbsp;<br>

<p>and scalar-matrix multiplication</p>

<p>&nbsp;<br>
$$
\boldsymbol{A}= \gamma\boldsymbol{B}  \Longrightarrow a_{ij} = \gamma b_{ij}.
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="vector-matrix-and-matrix-matrix-multiplication">Vector-matrix and Matrix-matrix multiplication </h2>

<p>We have also vector-matrix multiplications </p>
<p>&nbsp;<br>
$$
\boldsymbol{y}=\boldsymbol{Ax}   \Longrightarrow y_{i} = \sum_{j=0}^{n-1} a_{ij}x_j,
$$
<p>&nbsp;<br>

<p>and matrix-matrix multiplications</p>

<p>&nbsp;<br>
$$
\boldsymbol{A}=\boldsymbol{BC}   \Longrightarrow a_{ij} = \sum_{k=0}^{n-1} b_{ik}c_{kj},
$$
<p>&nbsp;<br>

<p>and transpositions of a matrix</p>

<p>&nbsp;<br>
$$
\boldsymbol{A}=\boldsymbol{B}^T   \Longrightarrow a_{ij} = b_{ji}.
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="important-mathematical-operations">Important Mathematical Operations </h2>

<p>Similarly, important vector operations that we will deal with are addition and subtraction</p>

<p>&nbsp;<br>
$$
\boldsymbol{x}= \boldsymbol{y}\pm\boldsymbol{z}  \Longrightarrow x_{i} = y_{i}\pm z_{i},
$$
<p>&nbsp;<br>

<p>scalar-vector multiplication</p>

<p>&nbsp;<br>
$$
\boldsymbol{x}= \gamma\boldsymbol{y}  \Longrightarrow x_{i} = \gamma y_{i},
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="other-important-mathematical-operations">Other important mathematical operations </h2>
<p>and vector-vector multiplication (called Hadamard multiplication)</p>
<p>&nbsp;<br>
$$
\boldsymbol{x}=\boldsymbol{yz}   \Longrightarrow x_{i} = y_{i}z_i.
$$
<p>&nbsp;<br>

<p>Finally, as already metnioned, the inner or so-called dot product  resulting in a constant</p>

<p>&nbsp;<br>
$$
x=\boldsymbol{y}^T\boldsymbol{z}   \Longrightarrow x = \sum_{j=0}^{n-1} y_{j}z_{j},
$$
<p>&nbsp;<br>

<p>and the outer product, which yields a matrix,</p>

<p>&nbsp;<br>
$$
\boldsymbol{A}=  \boldsymbol{y}\boldsymbol{z}^T \Longrightarrow  a_{ij} = y_{i}z_{j},
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="defining-basis-states-and-quantum-mechanical-operators">Defining basis states and quantum mechanical operators </h2>

<p>We extend now to quantum mechanics our definitions of vectors, matrices and more.</p>

<p>We start by defining a state vector \( \boldsymbol{x} \) (meant to represent
various quantum mechanical degrees of freedom) with \( n \) components as
</p>

<p>&nbsp;<br>
$$
\boldsymbol{x} = \begin{bmatrix} x_0\\ x_1 \\ x_2 \\ \dots \\ \dots \\ x_{n-1} \end{bmatrix}.
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="dirac-notation">Dirac notation </h2>

<p>Throughout these notes we will use the so-called Dirac <b>bra-ket</b>
formalism and we will replace the above standard boldfaced notation
for a vector with
</p>

<p>&nbsp;<br>
$$
\boldsymbol{x} = \vert x \rangle = \begin{bmatrix} x_0\\ x_1 \\ x_2 \\ \dots \\ \dots \\ x_{n-1} \end{bmatrix},
$$
<p>&nbsp;<br>

<p>and</p>
<p>&nbsp;<br>
$$
\boldsymbol{x}^{\dagger} = \langle x \vert = \begin{bmatrix} x_0^* & x_1^* & x_2^* & \dots & \dots & x_{n-1}^* \end{bmatrix},
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="inner-product-in-dirac-notation">Inner product in Dirac notation </h2>

<p>With a given vector \( \vert x \rangle \), we define the inner product as</p>
<p>&nbsp;<br>
$$
\langle x \vert x\rangle = \sum_{i=0}^{n-1} x_i^*x_i=x_0^2+x_1^2+\dots + x_{n-1}^2. 
$$
<p>&nbsp;<br>

<p>For two arbitrary vectors \( \vert x\rangle \) and \( \vert y\rangle \) with the same lentgh, we have the
general expression
</p>
<p>&nbsp;<br>
$$
\langle y \vert x\rangle = \sum_{i=0}^{n-1} y_i^*x_i=y_0^*x_0+y_1^*x_1+\dots + y_{n-1}^*x_{n-1}. 
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="the-inner-product-is-a-real-number">The inner product is a real number </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>Note well that the inner product \( \langle x \vert x\rangle \) is always a real number while for a two different vectors \( \langle y \vert x\rangle \) is in general not equal to
\( \langle x \vert y\rangle \), as can be seen from the example in the next slide. 
</p>
</div>


<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>We note in bypassing that \( \vert x\rangle^{\dagger}=\langle x \vert \),
\( \langle x\vert^{\dagger}=\vert x\rangle \) and \( (\vert
x\rangle^{\dagger})^{\dagger}=\vert x \rangle \).
</p>
</div>
</section>

<section>
<h2 id="examples">Examples </h2>

<p>Let us assume that \( \vert x \rangle \) is given by</p>
<p>&nbsp;<br>
$$
\vert x \rangle = \begin{bmatrix} 1-\imath \\ 2+\imath \end{bmatrix}.
$$
<p>&nbsp;<br>

<p>The inner product gives us</p>
<p>&nbsp;<br>
$$
\langle x\vert x \rangle = (1+\imath)(1-\imath)+(2-\imath)(2+\imath)=7,
$$
<p>&nbsp;<br>

<p>a real number.</p>
</section>

<section>
<h2 id="norm">Norm </h2>
<p>We can use the norm/inner product to normalize the vector \( \vert x \rangle \) and obtain</p>
<p>&nbsp;<br>
$$
\vert x \rangle = \frac{1}{\sqrt{7}}\begin{bmatrix} 1-\imath \\ 2+\imath \end{bmatrix}.
$$
<p>&nbsp;<br>

<p>As another example, consider the two vectors</p>
<p>&nbsp;<br>
$$
\vert x \rangle = \begin{bmatrix} -1 \\ 2\imath \\ 1\end{bmatrix},
$$
<p>&nbsp;<br>

<p>and</p>
<p>&nbsp;<br>
$$
\vert y \rangle = \begin{bmatrix} 1 \\ 0\imath \\ \imath\end{bmatrix}.
$$
<p>&nbsp;<br>

<p>We see that the inner products \( \langle x\vert y \rangle = -1+\imath \), which is not the same as
\( \langle y\vert x \rangle = -1-\imath \). This leads to the important rule
</p>
<p>&nbsp;<br>
$$
\langle x\vert y\rangle^* = \langle y \vert x\rangle. 
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="outer-products">Outer products </h2>

<p>In addition to inner products between vectors/states, the outer
product plays a central role in all of quantum mechanics. It is
defined as
</p>
<p>&nbsp;<br>
$$
\vert x\rangle \langle y \vert = \begin{bmatrix}
               x_0y_0^* & x_0y_1^* & x_0y_2^* & \dots & \dots & x_0y_{n-2}^* & x_0y_{n-1}^* \\
	       x_1y_0^* & x_1y_1^* & x_1y_2^* & \dots & \dots & x_1y_{n-2}^* & x_1y_{n-1}^* \\
	       x_2y_0^* & x_2y_1^* & x_2y_2^* & \dots & \dots & x_2y_{n-2}^* & x_2y_{n-1}^* \\	       
               \dots &   \dots   & \dots  & \dots & \dots & \dots & \dots \\
               \dots &   \dots   & \dots  & \dots & \dots & \dots & \dots \\	       
	       x_{n-2}y_0^* & x_{n-2}y_1^* & x_{n-2}y_2^* & \dots & \dots & x_{n-2}y_{n-2}^* & x_{n-2}y_{n-1}^* \\
	       x_{n-1}y_0^* & x_{n-1}y_1^* & x_{n-1}y_2^* & \dots & \dots & x_{n-1}y_{n-2}^* & x_{n-1}y_{n-1}^* \end{bmatrix}	       
$$
<p>&nbsp;<br>


<!-- to do: make figures with examples of basis states, hydrogen like systems, harmonic oscillator -->
<p>Assume we have a two-level system where the two states are represented
by the state vectors \( \vert \phi_0\rangle \) and \( \vert \phi_1\rangle \),
respectively. These states could represent selected or effective
degrees of freedom for either a single particle (fermion or boson) or
they could represent effective many-body degrees of freedon. In actual
realizations of quantum computing we search often for candidate
systems where we can use some low-lying states as computational basis
states. But we are not limited to quantum computing. When doing
many-body physics, due to the exploding degrees of freedom, we
normally search after effective ways by which we can reduce the
involved dimensionalities to a number of degrees of freedom we can
handle by a given many-body method.
</p>
<h3 id="examples-hydrogen-like-states-and-the-harmonic-oscillator-in-one-two-and-three-dimensions">Examples: Hydrogen like states and the harmonic oscillator in one, two and three dimensions </h3>

<p>We will now relabel the above two states as two orthogonal and normalized basis (ONB) states </p>
<p>&nbsp;<br>
$$
\vert \phi_0 \rangle = \vert 0 \rangle = \begin{bmatrix} 1 \\ 0 \end{bmatrix},
$$
<p>&nbsp;<br>

<p>and </p>
<p>&nbsp;<br>
$$
\vert \phi_1 \rangle = \vert 1 \rangle = \begin{bmatrix} 0 \\ 1 \end{bmatrix}.
$$
<p>&nbsp;<br>

<p>It is straight forward to see that \( \langle 1 \vert 0\rangle=0 \). With these two states we can define the define the identity operator \( \boldsymbol{I} \) as the sum of the outer products of these two states, namely</p>
<p>&nbsp;<br>
$$
\boldsymbol{I} = \sum_{i=0}^{i=1}\vert i\rangle \langle i\vert = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} +\begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}=\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}.
$$
<p>&nbsp;<br>

<p>We can further define the projection operators</p>
<p>&nbsp;<br>
$$
\boldsymbol{P} = \vert 0\rangle \langle 0\vert = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix},
$$
<p>&nbsp;<br>

<p>and </p>
<p>&nbsp;<br>
$$
\boldsymbol{Q} = \vert 1\rangle \langle 1\vert = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}.
$$
<p>&nbsp;<br>

<p>We note that \( P^2=P \), \( Q^2=Q \) (the operators are idempotent) and that
their determinants are zero, meaning in turn that we cannot use these
operators for unitary/orthogonal transformations. However, they play
important roles in defining effective Hilbert spaces for many-body
studies. Finally, before proceeding we note also that the two matrices
commute and we have \( \boldsymbol{P}\boldsymbol{Q}=0 \) and \( \left[ \boldsymbol{P},\boldsymbol{Q}\right]=0 \).
</p>
</section>

<section>
<h2 id="different-operators">Different operators </h2>

<p>The so-called Pauli matrices, and other simple
\( 2\times 2 \) matrices, play an important role, ranging from the setup
of quantum gates in quantum computing to a rewrite of creation and annihilation operators
and other quantum mechanical operators. Let us start with the familiar
Pauli matrices and remind ourselves of some of their basic properties.
</p>

<p>The Pauli matrices are defined as</p>
<p>&nbsp;<br>
$$
\sigma_x = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix},
$$
<p>&nbsp;<br>

<p>&nbsp;<br>
$$
\sigma_y = \begin{bmatrix} 0 & -\imath \\ \imath & 0 \end{bmatrix},
$$
<p>&nbsp;<br>

<p>and</p>
<p>&nbsp;<br>
$$
\sigma_z = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}.
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="properties-of-pauli-matrices">Properties of Pauli matrices </h2>

<p>It is easy to show that the matrices obey the properties (being involutory)</p>
<p>&nbsp;<br>
$$
\sigma_x\sigma_x = \sigma_y\sigma_y=\sigma_z\sigma_z = I=\begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix},
$$
<p>&nbsp;<br>

<p>that is their products with themselves result in the identity matrix
\( \boldsymbol{I} \).  Furthermore, the Pauli matrices are unitary matrices
meaning that their inverses are equal to their hermitian conjugated
matrices. The determinants of the Pauli matrices are all equal to \( -1 \),
as can be easily verified.
</p>
</section>

<section>
<h2 id="commutation-relations">Commutation relations </h2>

<p>The Pauli matrices obey also the following commutation rules</p>
<p>&nbsp;<br>
$$
\left[\sigma_x,\sigma_y\right] = 2\imath \sigma_z.
$$
<p>&nbsp;<br>

<p>Before we proceed with other matrices and how they can be used to
operate on various quantum mechanical states, let us try to define
various basis sets and their pertinent notations. We will often refer
to these basis states as our computational basis.
</p>
</section>

<section>
<h2 id="projection-operators">Projection operators </h2>

<p>We will now relabel the above two states as two orthogonal and normalized basis (ONB) states </p>
<p>&nbsp;<br>
$$
\vert \phi_0 \rangle = \vert 0 \rangle = \begin{bmatrix} 1 \\ 0 \end{bmatrix},
$$
<p>&nbsp;<br>

<p>and </p>
<p>&nbsp;<br>
$$
\vert \phi_1 \rangle = \vert 1 \rangle = \begin{bmatrix} 0 \\ 1 \end{bmatrix}.
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="identity-and-projection-operators">Identity and projection operators </h2>

<p>It is straight forward to see that \( \langle 1 \vert 0\rangle=0 \). With these two states we can define the define the identity operator \( \boldsymbol{I} \) as the sum of the outer products of these two states, namely</p>
<p>&nbsp;<br>
$$
\boldsymbol{I} = \sum_{i=0}^{i=1}\vert i\rangle \langle i\vert = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} +\begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}=\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}.
$$
<p>&nbsp;<br>

<p>We can further define the projection operators</p>
<p>&nbsp;<br>
$$
\boldsymbol{P} = \vert 0\rangle \langle 0\vert = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix},
$$
<p>&nbsp;<br>

<p>and </p>
<p>&nbsp;<br>
$$
\boldsymbol{Q} = \vert 1\rangle \langle 1\vert = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}.
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="idempotent-operators">Idempotent operators </h2>

<p>We note that \( P^2=P \), \( Q^2=Q \) (the operators are idempotent) and that
their determinants are zero, meaning in turn that we cannot use these
operators for unitary/orthogonal transformations. However, they play
important roles in defining effective Hilbert spaces for many-body
studies. Finally, before proceeding we note also that the two matrices
commute and we have \( \boldsymbol{P}\boldsymbol{Q}=0 \) and \( \left[ \boldsymbol{P},\boldsymbol{Q}\right]=0 \).
</p>
</section>

<section>
<h2 id="superposition-and-more">Superposition and more </h2>

<p>Using the properties of ONBs we can expand a new state in terms of the
above states. These states could also form  a basis which is an
eigenbasis of a selected Hamiltonian (more of this below).
</p>

<p>We define now a new state which is a linear expansion in terms of
these computational basis states
</p>

<p>&nbsp;<br>
$$
\vert \psi \rangle = \alpha \vert 0 \rangle + \beta\vert 1 \rangle,
$$
<p>&nbsp;<br>

<p>where the coefficients \( \alpha = \langle 0 \vert \psi \rangle \) and
\( \beta =\langle 1 \vert \psi\rangle \) reresent the overlaps between the
computational basis states and the state \( \vert \psi\rangle \). In quantum speech, we say the state is in a superposition of the states \( \vert 0\rangle \) and \( \vert 1\rangle \).
</p>
</section>

<section>
<h2 id="inner-products">Inner products </h2>
<p>Computing the inner product of \( \vert \psi \rangle \) we obtain</p>
<p>&nbsp;<br>
$$
\langle \psi \vert \psi \rangle = \vert \alpha \vert ^2\langle 0\vert 0\rangle + \vert \beta \vert ^2\langle 1\vert 1\rangle = \vert \alpha \vert ^2 + \vert \beta \vert ^2 = 1,
$$
<p>&nbsp;<br>

<p>since the new basis, which is defined in terms of a a unitary/orthogonal
transformation, preserves the orthogonality and norm of the original
computational basis \( \vert 0\rangle \) and \( \vert 1\rangle \). To see
this, consider the unitary transformation (show derivation of
preserving orthogonality).
</p>
</section>

<section>
<h2 id="acting-with-projection-operators">Acting with projection operators </h2>

<p>If we now act with the projection operators \( \boldsymbol{P} \) and \( \boldsymbol{Q} \) on
the state \( \vert \psi\rangle \) we get
</p>

<p>&nbsp;<br>
$$
\boldsymbol{P}\vert \psi \rangle = \vert 0 \rangle\langle 0\vert (\alpha \vert 0 \rangle + \beta\vert 1 \rangle)=\alpha \vert 0\rangle,
$$
<p>&nbsp;<br>

<p>that is we <b>project</b> out the \( \vert 0\rangle \) component of the state
\( \vert \psi\rangle \) with the coefficient \( \alpha \) while \( \boldsymbol{Q} \)
projects out the \( \vert 1\rangle \) component with coefficient \( \beta \)
as seen from
</p>

<p>&nbsp;<br>
$$
\boldsymbol{Q}\vert \psi \rangle = \vert 1 \rangle\langle 1\vert (\alpha \vert 0 \rangle + \beta\vert 1 \rangle)=\beta \vert 1\rangle.
$$
<p>&nbsp;<br>

<p>The above results can easily be derived by multiplying the pertinent
matrices with the vectors \( \vert 0\rangle \) and \( \vert 1\rangle \),
respectively.
</p>
</section>

<section>
<h2 id="density-matrix">Density matrix </h2>

<p>Using the above linear expansion we can now define the density matrix of the state \( \vert \psi\rangle \) as the outer product</p>
<p>&nbsp;<br>
$$
\boldsymbol{\rho}=\vert \psi \rangle\langle \psi \vert = \alpha\alpha^* \vert 0 \rangle\langle 0\vert+\alpha\beta^* \vert 0 \rangle\langle 1\vert+\beta\alpha^* \vert 1 \rangle\langle 0\vert+\beta\beta^* \vert 1 \rangle\langle 1\vert,
$$
<p>&nbsp;<br>

<p>which leads to</p>
<p>&nbsp;<br>
$$
\boldsymbol{\rho}=\begin{bmatrix} \alpha\alpha^* & \alpha\beta^*\\ \beta\alpha^* & \beta\beta^*\end{bmatrix}.
$$
<p>&nbsp;<br>

<p>Finally, we note that the trace of the density matrix is simply given by unity</p>
<p>&nbsp;<br>
$$
\mathrm{tr}\boldsymbol{\rho}=\alpha\alpha^* +\beta\beta^*=1.
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="tensor-products">Tensor products </h2>

<p>Consider now two vectors with length \( n=2 \), with elements</p>

<p>&nbsp;<br>
$$
\vert x \rangle = \begin{bmatrix} x_0 \\ x_1 \end{bmatrix}, 
$$
<p>&nbsp;<br>

<p>and</p>
<p>&nbsp;<br>
$$
\vert y \rangle = \begin{bmatrix} y_0 \\ y_1 \end{bmatrix}. 
$$
<p>&nbsp;<br>

<p>The tensor product of these two vectors is defined as</p>
<p>&nbsp;<br>
$$
\vert x \rangle \otimes \vert y \rangle = \vert xy \rangle  = \begin{bmatrix} x_0y_0 \\ x_0y_1 \\ x_1y_0 \\ x_1y_1 \end{bmatrix}, 
$$
<p>&nbsp;<br>

<p>which is now a vector of length \( 4 \).</p>
</section>

<section>
<h2 id="examples-of-tensor-products">Examples of tensor products </h2>
<p>If we now go back to our original one-qubit basis states, we can form teh following tensor products</p>
<p>&nbsp;<br>
$$
\vert 0 \rangle \otimes \vert 0 \rangle = \begin{bmatrix} 1 \\ 0\end{bmatrix} \otimes \begin{bmatrix} 1 \\ 0\end{bmatrix} =\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}=\vert 00 \rangle, 
$$
<p>&nbsp;<br>

<p>&nbsp;<br>
$$
\vert 0 \rangle \otimes \vert 1 \rangle = \begin{bmatrix} 1 \\ 0\end{bmatrix} \otimes \begin{bmatrix} 0 \\ 1\end{bmatrix} =\begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}=\vert 01 \rangle.
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="more-states">More states </h2>

<p>&nbsp;<br>
$$
\vert 1 \rangle \otimes \vert 0 \rangle = \begin{bmatrix} 0 \\ 1\end{bmatrix} \otimes \begin{bmatrix} 1 \\ 0\end{bmatrix} =\begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \end{bmatrix}=\vert 10 \rangle, 
$$
<p>&nbsp;<br>

<p>and finally</p>
<p>&nbsp;<br>
$$
\vert 1 \rangle \otimes \vert 1 \rangle = \begin{bmatrix} 0 \\ 1\end{bmatrix} \otimes \begin{bmatrix} 0 \\ 1\end{bmatrix} =\begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \end{bmatrix}=\vert 11 \rangle. 
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="measurements">Measurements </h2>

<p>The probability of a measurement on a quantum system giving a certain
result is determined by the weight of the relevant basis state in the
state vector. After the measurement, the system is in a state that
corresponds to the result of the measurement. The operators and
gates discussed below are examples of operations we can perform on
specific states.
</p>

<p>We  consider the state</p>
<p>&nbsp;<br>
$$
\vert \psi\rangle = \alpha \vert 0 \rangle +\beta \vert 1 \rangle
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="definitions-of-measurements">Definitions of measurements </h2>

<ol>
<p><li> A measurement can yield only one of the above states, either \( \vert 0\rangle \) or \( \vert 1\rangle \).</li>
<p><li> The probability of a measurement resulting in \( \vert 0\rangle \) is \( \alpha^*\alpha = \vert \alpha \vert^2 \).</li>
<p><li> The probability of a measurement resulting in \( \vert 1\rangle \) is \( \beta^*\beta = \vert \beta \vert^2 \).</li>
<p><li> And we note that the sum of the outcomes gives \( \alpha^*\alpha+\beta^*\beta=1 \) since the two states are normalized.</li>
</ol>
<p>
<p>After the measurement, the state of the system is the state associated with the result of the measurement.</p>

<p>We have already encountered the projection operators \( P \) and \( Q \). Let
us now look at other types of operations we can make on qubit states.
</p>
</section>

<section>
<h2 id="different-operators-and-gates">Different operators and gates </h2>

<p>In quantum computing, the so-called Pauli matrices, and other simple
\( 2\times 2 \) matrices, play an important role, ranging from the setup
of quantum gates to a rewrite of creation and annihilation operators
and other quantum mechanical operators. Let us start with the familiar
Pauli matrices and remind ourselves of some of their basic properties.
</p>

<p>Assume we operate with \( \sigma_x \) on our basis state \( \vert 0 \rangle \). This gives</p>
<p>&nbsp;<br>
$$
\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}\begin{bmatrix} 1 \\ 0 \end{bmatrix}=\begin{bmatrix} 0  \\ 1  \end{bmatrix},
$$
<p>&nbsp;<br>

<p>that is we switch from \( \vert 0\rangle \) to \( \vert 1\rangle \) (often called a spin flip operation) and similary we have</p>
<p>&nbsp;<br>
$$
\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}\begin{bmatrix} 0 \\ 1 \end{bmatrix}=\begin{bmatrix} 1  \\ 0  \end{bmatrix}.
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="more-on-pauli-matrices">More on Pauli matrices </h2>

<p>This matrix plays an important role in quantum computing since we can
link this with the classical <b>NOT</b> operation.  If we send in bit 0,
the <b>NOT</b> gate outputs bit 1 and vice versa. We can use the \( \sigma_x \)
matrix to implement the quantum mechanical equivalent of a classical
<b>NOT</b> gate. If we input what we could represent as bit 0 in terms of
the basis state \( \vert 0\rangle \), operating on this state results in
the state \( \vert 1\rangle \), which we in turn can interpret as the
classical bit 1.
</p>
</section>

<section>
<h2 id="linear-superposition">Linear superposition </h2>
<p>If we have a linear superposition of these states we obtain</p>
<p>&nbsp;<br>
$$
\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix}\begin{bmatrix}\alpha \\ \beta \end{bmatrix}=\begin{bmatrix}\beta \\ \alpha \end{bmatrix}.
$$
<p>&nbsp;<br>

<p>The \( \sigma_y \) matrix introduces an imaginary sign, which we will later encounter in terms of so-called phase-shift operations.</p>
</section>

<section>
<h2 id="the-sigma-z-matrix">The \( \sigma_z \) matrix </h2>
<p>The \( \sigma_z \) matrix has the following effect</p>
<p>&nbsp;<br>
$$
\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}\begin{bmatrix} 1 \\ 0 \end{bmatrix}=\begin{bmatrix} 1  \\ 0  \end{bmatrix},
$$
<p>&nbsp;<br>

<p>and </p>
<p>&nbsp;<br>
$$
\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}\begin{bmatrix} 0 \\ 1 \end{bmatrix}=\begin{bmatrix} 0  \\ -1  \end{bmatrix},
$$
<p>&nbsp;<br>

<p>which we will also link with a specific phase-shift.</p>
</section>

<section>
<h2 id="unitarity">Unitarity </h2>

<p>The matrices we introduced here are so-called unitary matrices. This
is an important element in quantum mechanics since the evolution of a
closed quantum system is described by operations involving unitary
operations only.
</p>

<p>We have defined a new state \( \vert \psi_p\rangle \) as a linear expansion in terms of an orthogonal and normalized basis (our computational basis) \( \phi_{\lambda} \)</p>
<p>&nbsp;<br>
$$
\begin{equation}
\vert \psi_i\rangle = \sum_{j} u_{ij}\vert \phi_{j}\rangle.
\tag{1}
\end{equation}
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="hamiltonians-and-basis-functions">Hamiltonians and basis functions </h2>

<p>It is normal to choose a basis defined as the eigenfunctions of parts
of the full Hamiltonian. The typical situation consists of the
solutions of the one-body part of the Hamiltonian, that is we have
</p>

<p>&nbsp;<br>
$$
\hat{h}_0\vert \phi_{i}\rangle=\epsilon_{i}\vert \phi_{i}\rangle.
$$
<p>&nbsp;<br>

<p>This is normally referred to as a single-particle basis \( \vert\phi_{i}(\mathbf{r})\rangle \),
defined by the quantum numbers \( i \) and \( \mathbf{r} \).
</p>
</section>

<section>
<h2 id="unitary-transformations">Unitary transformations </h2>
<p>A unitary transformation is important since it keeps the orthogonality.
To see this consider first a basis of vectors \( \mathbf{v}_i \),
</p>
<p>&nbsp;<br>
$$
\mathbf{v}_i = \begin{bmatrix} v_{i1} \\ \dots \\ \dots \\v_{in} \end{bmatrix}
$$
<p>&nbsp;<br>

<p>We assume that the basis is orthogonal, that is </p>
<p>&nbsp;<br>
$$
\mathbf{v}_j^T\mathbf{v}_i = \delta_{ij}.
$$
<p>&nbsp;<br>

<p>An orthogonal or unitary transformation</p>
<p>&nbsp;<br>
$$
\mathbf{w}_i=\mathbf{U}\mathbf{v}_i,
$$
<p>&nbsp;<br>

<p>preserves the dot product and orthogonality since</p>
<p>&nbsp;<br>
$$
\mathbf{w}_j^T\mathbf{w}_i=(\mathbf{U}\mathbf{v}_j)^T\mathbf{U}\mathbf{v}_i=\mathbf{v}_j^T\mathbf{U}^T\mathbf{U}\mathbf{v}_i= \mathbf{v}_j^T\mathbf{v}_i = \delta_{ij}.
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="orthogonality-preserved">Orthogonality preserved </h2>

<p>This means that if the coefficients \( u_{p\lambda} \) belong to a unitary
or orthogonal transformation (using the Dirac bra-ket notation)
</p>

<p>&nbsp;<br>
$$
\vert \psi_i\rangle = \sum_{j} u_{ij}\vert \phi_{j}\rangle.
$$
<p>&nbsp;<br>

<p>orthogonality is preserved.</p>

<p>This propertry is extremely useful when we build up a basis of
many-body determinant based states.
</p>

<p>Note also that although a basis \( \left\{\vert \phi_i \rangle\right\} \) contains an infinity of states, for practical calculations we have always to make some truncations.</p>
</section>

<section>
<h2 id="example">Example </h2>

<p>Assume we have two one-qubit states represented by</p>
<p>&nbsp;<br>
$$
\vert \psi \rangle = \alpha \vert 0 \rangle + \beta \vert 1\rangle=\begin{bmatrix}\alpha \\ \beta \end{bmatrix},
$$
<p>&nbsp;<br>

<p>and</p>
<p>&nbsp;<br>
$$
\vert \phi \rangle = \gamma \vert 0 \rangle + \delta \vert 1\rangle=\begin{bmatrix}\gamma \\ \delta \end{bmatrix}.
$$
<p>&nbsp;<br>

<p>We assume that the state \( \vert \phi \rangle \) is obtained through a
unitary transformation of \( \vert \psi \rangle \) through a matrix
\( \boldsymbol{U} \) with its hermitian conjugate \( \boldsymbol{U}^{\dagger} \) with matrix
elements \( u_{ij}^{\dagger}=u_{ji}^* \) and
\( \boldsymbol{I}=\boldsymbol{U}\boldsymbol{U}^{\dagger}=\boldsymbol{U}^{\dagger}\boldsymbol{U} \).
</p>
</section>

<section>
<h2 id="inverse-of-unitary-matrices">Inverse of unitary matrices </h2>

<p>Note that this means that the hermitian conjugate of a unitary matrix
is equal to its inverse. This has important consequences for what is
called reversibility. We say quantum mechanics is a theory which is
reversible with a probabilistic determinism. Classical mechanics on
the other is reversible in a deterministic way, that is, knowing all
initial conditions we can in principle determine the future motion of
an object which obey the laws of motion of classical mechanics.
</p>

<p>We have then</p>
<p>&nbsp;<br>
$$
\begin{bmatrix}\gamma \\ \delta \end{bmatrix}=\begin{bmatrix}u_{00} & u_{01} \\ u_{10} & u_{11} \end{bmatrix}\begin{bmatrix}\alpha \\ \beta \end{bmatrix}.
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="new-basis-is-also-orthogonal">New basis is also orthogonal </h2>

<p>Since our original basis \( \vert \psi\rangle \) is orthogonal and normalized with \( \vert\alpha\vert^2+\vert\beta\vert^2=1 \), the new basis is also orthogonal and normalized, as we can see below here.</p>

<p>Since the inverse of a hermitian matrix is equal to its hermitian
conjugate/adjoint), unitary transformations are always reversible.
</p>

<p>Why are only unitary transformations allowed? The key lies in the way the inner product tranforms.</p>

<p>To see this we rewrite the new basis from the previous example in its two components as</p>
<p>&nbsp;<br>
$$
\vert \phi\rangle_i=\sum_{j}u_{ij}\vert \psi\rangle_j,
$$
<p>&nbsp;<br>

<p>or in terms of a matrix-vector notatio we have</p>
<p>&nbsp;<br>
$$
\vert \phi\rangle=\boldsymbol{U}\vert \psi\rangle,
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="more-on-orthogonality">More on orthogonality </h2>

<p>We have already assumed that \( \langle \psi \vert \psi \rangle = \vert\alpha\vert^2+\vert\beta\vert^2=1 \).</p>

<p>We have that </p>
<p>&nbsp;<br>
$$
\langle \phi\vert_i=\sum_{j}u_{ij}^*\langle \psi\vert_j,
$$
<p>&nbsp;<br>

<p>or in terms of a matrix-vector notation we have</p>
<p>&nbsp;<br>
$$
\langle \phi\vert=\langle \psi\vert\boldsymbol{U}^{\dagger}.
$$
<p>&nbsp;<br>

<p>Note that the two vectors are row vectors now.</p>

<p>If we stay with this notation we have</p>

<p>&nbsp;<br>
$$
\langle \phi\vert\phi\rangle = \langle \psi \boldsymbol{U}^{\dagger}\boldsymbol{U}\vert \psi\rangle = \langle \psi\vert \psi\rangle=1!
$$
<p>&nbsp;<br>

<p>Unitary transformations are rotations in state space which preserve the
length (the square root of the inner product) of the state vector.
</p>
<h2 id="hamiltonian-and-more-definition">Hamiltonian and more definition </h2>

<p>Before we proceed we need several definitions.  Throughout these
lectures we will assume that the interacting part of the Hamiltonian
can be approximated by a two-body interaction.  This means that our
Hamiltonian can be written as the sum of a onebody part, which
includes kinetic energy and an eventual external field, and a twobody
interaction
</p>

<p>This means that our Hamiltonian is written as the sum of some onebody part and a twobody part</p>
<p>&nbsp;<br>
$$
\begin{equation}
    \hat{H} = \hat{H}_0 + \hat{H}_I 
    = \sum_{i=1}^N \hat{h}_0(x_i) + \sum_{i < j}^N \hat{v}(r_{ij}),
\tag{2}
\end{equation}
$$
<p>&nbsp;<br>

<p>with </p>
<p>&nbsp;<br>
$$
\begin{equation}
  H_0=\sum_{i=1}^N \hat{h}_0(x_i).
\tag{3}
\end{equation}
$$
<p>&nbsp;<br>

<p>The onebody part \( u_{\mathrm{ext}}(x_i) \) is normally approximated by a harmonic oscillator potential or the Coulomb interaction an electron feels from the nucleus. However, other potentials are fully possible, such as 
one derived from the self-consistent solution of the Hartree-Fock equations to be discussed here.
</p>

<p>Our Hamiltonian is invariant under the permutation (interchange) of two particles.
Since we mainly will deal with fermions however, the total wave function is antisymmetric.
Let \( \hat{P} \) be an operator which interchanges two particles.
Due to the symmetries we have ascribed to our Hamiltonian, this operator commutes with the total Hamiltonian,
</p>
<p>&nbsp;<br>
$$
[\hat{H},\hat{P}] = 0,
 $$
<p>&nbsp;<br>

<p>meaning that \( \Psi_{\lambda}(x_1, x_2, \dots , x_N) \) is an eigenfunction of 
\( \hat{P} \) as well, that is
</p>
<p>&nbsp;<br>
$$
\hat{P}_{ij}\Psi_{\lambda}(x_1, x_2, \dots,x_i,\dots,x_j,\dots,x_N)=
\beta\Psi_{\lambda}(x_1, x_2, \dots,x_i,\dots,x_j,\dots,x_N),
$$
<p>&nbsp;<br>

<p>where \( \beta \) is the eigenvalue of \( \hat{P} \). We have introduced the suffix \( ij \) in order to indicate that we permute particles \( i \) and \( j \).
The Pauli principle tells us that the total wave function for a system of fermions
has to be antisymmetric, resulting in the eigenvalue \( \beta = -1 \).   
</p>

<p>In our case we assume that  we can approximate the exact eigenfunction with a <b>so-called</b> <a href="https://chem.libretexts.org/Courses/Pacific_Union_College/Quantum_Chemistry/08%3A_Multielectron_Atoms/8.06%3A_Antisymmetric_Wavefunctions_can_be_Represented_by_Slater_Determinants" target="_blank">Slater determinant</a></p>
<p>&nbsp;<br>
$$
\begin{equation}
   \Phi(x_1, x_2,\dots ,x_N,\alpha,\beta,\dots, \sigma)=\frac{1}{\sqrt{N!}}
\left| \begin{array}{ccccc} \psi_{\alpha}(x_1)& \psi_{\alpha}(x_2)& \dots & \dots & \psi_{\alpha}(x_N)\\
                            \psi_{\beta}(x_1)&\psi_{\beta}(x_2)& \dots & \dots & \psi_{\beta}(x_N)\\  
                            \dots & \dots & \dots & \dots & \dots \\
                            \dots & \dots & \dots & \dots & \dots \\
                     \psi_{\sigma}(x_1)&\psi_{\sigma}(x_2)& \dots & \dots & \psi_{\sigma}(x_N)\end{array} \right|, \tag{4}
\end{equation}
$$
<p>&nbsp;<br>

<p>where  \( x_i \)  stand for the coordinates and spin values of a particle \( i \) and \( \alpha,\beta,\dots, \gamma \) 
are quantum numbers needed to describe remaining quantum numbers.  
</p>

<p>Since we will mainly deal with Fermions (identical and indistinguishable particles) we will 
form an ansatz for a given state in terms of so-called Slater determinants determined
by a chosen basis of single-particle functions. 
</p>

<p>For a given \( n\times n \) matrix \( \mathbf{A} \) we can write its determinant</p>
<p>&nbsp;<br>
$$
   det(\mathbf{A})=|\mathbf{A}|=
\left| \begin{array}{ccccc} a_{11}& a_{12}& \dots & \dots & a_{1n}\\
                            a_{21}&a_{22}& \dots & \dots & a_{2n}\\  
                            \dots & \dots & \dots & \dots & \dots \\
                            \dots & \dots & \dots & \dots & \dots \\
                            a_{n1}& a_{n2}& \dots & \dots & a_{nn}\end{array} \right|,
$$
<p>&nbsp;<br>

<p>in a more compact form as </p>
<p>&nbsp;<br>
$$
|\mathbf{A}|= \sum_{i=1}^{n!}(-1)^{p_i}\hat{P}_i a_{11}a_{22}\dots a_{nn},
$$
<p>&nbsp;<br>

<p>where \( \hat{P}_i \) is a permutation operator which permutes the column indices \( 1,2,3,\dots,n \)
and the sum runs over all \( n! \) permutations.  The quantity \( p_i \) represents the number of transpositions of column indices that are needed in order to bring a given permutation back to its initial ordering, in our case given by \( a_{11}a_{22}\dots a_{nn} \) here.
</p>

<p>A simple \( 2\times 2 \) determinant illustrates this. We have</p>
<p>&nbsp;<br>
$$
   det(\mathbf{A})=
\left| \begin{array}{cc} a_{11}& a_{12}\\
                            a_{21}&a_{22}\end{array} \right|= (-1)^0a_{11}a_{22}+(-1)^1a_{12}a_{21},
$$
<p>&nbsp;<br>

<p>where in the last term we have interchanged the column indices \( 1 \) and \( 2 \). The natural ordering we have chosen is \( a_{11}a_{22} \). </p>
<h3 id="back-to-the-derivation-of-the-energy">Back to the derivation of the energy </h3>

<p>The single-particle function \( \psi_{\alpha}(x_i) \)  are eigenfunctions of the onebody
Hamiltonian \( h_i \), that is
</p>
<p>&nbsp;<br>
$$
\hat{h}_0(x_i)=\hat{t}(x_i) + \hat{u}_{\mathrm{ext}}(x_i),
$$
<p>&nbsp;<br>

<p>with eigenvalues </p>
<p>&nbsp;<br>
$$
\hat{h}_0(x_i) \psi_{\alpha}(x_i)=\left(\hat{t}(x_i) + \hat{u}_{\mathrm{ext}}(x_i)\right)\psi_{\alpha}(x_i)=\varepsilon_{\alpha}\psi_{\alpha}(x_i).
$$
<p>&nbsp;<br>

<p>The energies \( \varepsilon_{\alpha} \) are the so-called non-interacting single-particle energies, or unperturbed energies. 
The total energy is in this case the sum over all  single-particle energies, if no two-body or more complicated
many-body interactions are present.
</p>

<p>Let us denote the ground state energy by \( E_0 \). According to the
variational principle we have
</p>
<p>&nbsp;<br>
$$
  E_0 \le E[\Phi] = \int \Phi^*\hat{H}\Phi d\mathbf{\tau}
$$
<p>&nbsp;<br>

<p>where \( \Phi \) is a trial function which we assume to be normalized</p>
<p>&nbsp;<br>
$$
  \int \Phi^*\Phi d\mathbf{\tau} = 1,
$$
<p>&nbsp;<br>

<p>where we have used the shorthand \( d\mathbf{\tau}=dx_1dr_2\dots dr_N \).</p>

<p>In the Hartree-Fock method the trial function is the Slater
determinant of Eq.&nbsp;<a href="#mjx-eqn-4">(4)</a> which can be rewritten as 
</p>
<p>&nbsp;<br>
$$
  \Phi(x_1,x_2,\dots,x_N,\alpha,\beta,\dots,\nu) = \frac{1}{\sqrt{N!}}\sum_{P} (-)^P\hat{P}\psi_{\alpha}(x_1)
    \psi_{\beta}(x_2)\dots\psi_{\nu}(x_N)=\sqrt{N!}\hat{A}\Phi_H,
$$
<p>&nbsp;<br>

<p>where we have introduced the antisymmetrization operator \( \hat{A} \) defined by the 
summation over all possible permutations of two particles.
</p>

<p>It is defined as</p>
<p>&nbsp;<br>
$$
\begin{equation}
  \hat{A} = \frac{1}{N!}\sum_{p} (-)^p\hat{P},
\tag{5}
\end{equation}
$$
<p>&nbsp;<br>

<p>with \( p \) standing for the number of permutations. We have introduced for later use the so-called
Hartree-function, defined by the simple product of all possible single-particle functions
</p>
<p>&nbsp;<br>
$$
  \Phi_H(x_1,x_2,\dots,x_N,\alpha,\beta,\dots,\nu) =
  \psi_{\alpha}(x_1)
    \psi_{\beta}(x_2)\dots\psi_{\nu}(x_N).
$$
<p>&nbsp;<br>

<p>Both \( \hat{H}_0 \) and \( \hat{H}_I \) are invariant under all possible permutations of any two particles
and hence commute with \( \hat{A} \)
</p>
<p>&nbsp;<br>
$$
\begin{equation}
  [H_0,\hat{A}] = [H_I,\hat{A}] = 0. \tag{6}
\end{equation}
$$
<p>&nbsp;<br>

<p>Furthermore, \( \hat{A} \) satisfies</p>
<p>&nbsp;<br>
$$
\begin{equation}
  \hat{A}^2 = \hat{A},  \tag{7}
\end{equation}
$$
<p>&nbsp;<br>

<p>since every permutation of the Slater
determinant reproduces it. 
</p>

<p>The expectation value of \( \hat{H}_0 \) </p>
<p>&nbsp;<br>
$$
  \int \Phi^*\hat{H}_0\Phi d\mathbf{\tau} 
  = N! \int \Phi_H^*\hat{A}\hat{H}_0\hat{A}\Phi_H d\mathbf{\tau}
$$
<p>&nbsp;<br>

<p>is readily reduced to</p>
<p>&nbsp;<br>
$$
  \int \Phi^*\hat{H}_0\Phi d\mathbf{\tau} 
  = N! \int \Phi_H^*\hat{H}_0\hat{A}\Phi_H d\mathbf{\tau},
$$
<p>&nbsp;<br>

<p>where we have used Eqs.&nbsp;<a href="#mjx-eqn-6">(6)</a> and
<a href="#mjx-eqn-7">(7)</a>. The next step is to replace the antisymmetrization
operator by its definition and to
replace \( \hat{H}_0 \) with the sum of one-body operators
</p>
<p>&nbsp;<br>
$$
  \int \Phi^*\hat{H}_0\Phi  d\mathbf{\tau}
  = \sum_{i=1}^N \sum_{p} (-)^p\int 
  \Phi_H^*\hat{h}_0\hat{P}\Phi_H d\mathbf{\tau}.
$$
<p>&nbsp;<br>

<p>The integral vanishes if two or more particles are permuted in only one
of the Hartree-functions \( \Phi_H \) because the individual single-particle wave functions are
orthogonal. We obtain then
</p>
<p>&nbsp;<br>
$$
  \int \Phi^*\hat{H}_0\Phi  d\mathbf{\tau}= \sum_{i=1}^N \int \Phi_H^*\hat{h}_0\Phi_H  d\mathbf{\tau}.
$$
<p>&nbsp;<br>

<p>Orthogonality of the single-particle functions allows us to further simplify the integral, and we
arrive at the following expression for the expectation values of the
sum of one-body Hamiltonians 
</p>
<p>&nbsp;<br>
$$
\begin{equation}
  \int \Phi^*\hat{H}_0\Phi  d\mathbf{\tau}
  = \sum_{\mu=1}^N \int \psi_{\mu}^*(x)\hat{h}_0\psi_{\mu}(x)dx
  d\mathbf{r}.
\tag{8}
\end{equation}
$$
<p>&nbsp;<br>

<p>We introduce the following shorthand for the above integral</p>
<p>&nbsp;<br>
$$
\langle \mu | \hat{h}_0 | \mu \rangle = \int \psi_{\mu}^*(x)\hat{h}_0\psi_{\mu}(x)dx,
$$
<p>&nbsp;<br>

<p>and rewrite Eq.&nbsp;<a href="#mjx-eqn-8">(8)</a> as</p>
<p>&nbsp;<br>
$$
\begin{equation}
  \int \Phi^*\hat{H}_0\Phi  d\tau
  = \sum_{\mu=1}^N \langle \mu | \hat{h}_0 | \mu \rangle.
\tag{9}
\end{equation}
$$
<p>&nbsp;<br>

<p>The expectation value of the two-body part of the Hamiltonian is obtained in a
similar manner. We have
</p>
<p>&nbsp;<br>
$$
  \int \Phi^*\hat{H}_I\Phi d\mathbf{\tau} 
  = N! \int \Phi_H^*\hat{A}\hat{H}_I\hat{A}\Phi_H d\mathbf{\tau},
$$
<p>&nbsp;<br>

<p>which reduces to</p>
<p>&nbsp;<br>
$$
 \int \Phi^*\hat{H}_I\Phi d\mathbf{\tau} 
  = \sum_{i\le j=1}^N \sum_{p} (-)^p\int 
  \Phi_H^*\hat{v}(r_{ij})\hat{P}\Phi_H d\mathbf{\tau},
$$
<p>&nbsp;<br>

<p>by following the same arguments as for the one-body
Hamiltonian. 
</p>

<p>Because of the dependence on the inter-particle distance \( r_{ij} \),  permutations of
any two particles no longer vanish, and we get
</p>
<p>&nbsp;<br>
$$
  \int \Phi^*\hat{H}_I\Phi d\mathbf{\tau} 
  = \sum_{i < j=1}^N \int  
  \Phi_H^*\hat{v}(r_{ij})(1-P_{ij})\Phi_H d\mathbf{\tau}.
$$
<p>&nbsp;<br>

<p>where \( P_{ij} \) is the permutation operator that interchanges
particle \( i \) and particle \( j \). Again we use the assumption that the single-particle wave functions
are orthogonal. 
</p>

<p>We obtain</p>
<p>&nbsp;<br>
$$
\begin{align}
  \int \Phi^*\hat{H}_I\Phi d\mathbf{\tau} 
  = \frac{1}{2}\sum_{\mu=1}^N\sum_{\nu=1}^N
    &\left[ \int \psi_{\mu}^*(x_i)\psi_{\nu}^*(x_j)\hat{v}(r_{ij})\psi_{\mu}(x_i)\psi_{\nu}(x_j)
    dx_idx_j \right.
\tag{10}\\
  &\left.
  - \int \psi_{\mu}^*(x_i)\psi_{\nu}^*(x_j)
  \hat{v}(r_{ij})\psi_{\nu}(x_i)\psi_{\mu}(x_j)
  dx_idx_j
  \right]. \tag{11}
\end{align}
$$
<p>&nbsp;<br>

<p>The first term is the so-called direct term. It is frequently also called the  Hartree term, 
while the second is due to the Pauli principle and is called
the exchange term or just the Fock term.
The factor  \( 1/2 \) is introduced because we now run over
all pairs twice. 
</p>

<p>The last equation allows us to  introduce some further definitions.  
The single-particle wave functions \( \psi_{\mu}(x) \), defined by the quantum numbers \( \mu \) and \( x \)
are defined as the overlap 
</p>
<p>&nbsp;<br>
$$
   \psi_{\alpha}(x)  = \langle x | \alpha \rangle .
$$
<p>&nbsp;<br>

<p>We introduce the following shorthands for the above two integrals</p>
<p>&nbsp;<br>
$$
\langle \mu\nu|\hat{v}|\mu\nu\rangle =  \int \psi_{\mu}^*(x_i)\psi_{\nu}^*(x_j)\hat{v}(r_{ij})\psi_{\mu}(x_i)\psi_{\nu}(x_j)
    dx_idx_j,
$$
<p>&nbsp;<br>

<p>and</p>
<p>&nbsp;<br>
$$
\langle \mu\nu|\hat{v}|\nu\mu\rangle = \int \psi_{\mu}^*(x_i)\psi_{\nu}^*(x_j)
  \hat{v}(r_{ij})\psi_{\nu}(x_i)\psi_{\mu}(x_j)
  dx_idx_j.  
$$
<p>&nbsp;<br>
<h2 id="preparing-for-later-studies-varying-the-coefficients-of-a-wave-function-expansion-and-orthogonal-transformations">Preparing for later studies: varying the coefficients of a wave function expansion and orthogonal transformations </h2>

<p>It is common to  expand the single-particle functions in a known basis  and vary the coefficients, 
that is, the new single-particle wave function is written as a linear expansion
in terms of a fixed chosen orthogonal basis (for example the well-known harmonic oscillator functions or the hydrogen-like functions etc).
We define our new single-particle basis (this is a normal approach for Hartree-Fock theory) by performing a unitary transformation 
on our previous basis (labelled with greek indices) as
</p>
<p>&nbsp;<br>
$$
\begin{equation}
\psi_p^{new}  = \sum_{\lambda} C_{p\lambda}\phi_{\lambda}. \tag{12}
\end{equation}
$$
<p>&nbsp;<br>

<p>In this case we vary the coefficients \( C_{p\lambda} \). If the basis has infinitely many solutions, we need
to truncate the above sum.  We assume that the basis \( \phi_{\lambda} \) is orthogonal.
</p>

<p>It is normal to choose a single-particle basis defined as the eigenfunctions
of parts of the full Hamiltonian. The typical situation consists of the solutions of the one-body part of the Hamiltonian, that is we have
</p>
<p>&nbsp;<br>
$$
\hat{h}_0\phi_{\lambda}=\epsilon_{\lambda}\phi_{\lambda}.
$$
<p>&nbsp;<br>

<p>The single-particle wave functions \( \phi_{\lambda}(\mathbf{r}) \), defined by the quantum numbers \( \lambda \) and \( \mathbf{r} \)
are defined as the overlap 
</p>
<p>&nbsp;<br>
$$
   \phi_{\lambda}(\mathbf{r})  = \langle \mathbf{r} | \lambda \rangle .
$$
<p>&nbsp;<br>

<p>In deriving the Hartree-Fock equations, we  will expand the single-particle functions in a known basis  and vary the coefficients, 
that is, the new single-particle wave function is written as a linear expansion
in terms of a fixed chosen orthogonal basis (for example the well-known harmonic oscillator functions or the hydrogen-like functions etc).
</p>

<p>We stated that a unitary transformation keeps the orthogonality. To see this consider first a basis of vectors \( \mathbf{v}_i \),</p>
<p>&nbsp;<br>
$$
\mathbf{v}_i = \begin{bmatrix} v_{i1} \\ \dots \\ \dots \\v_{in} \end{bmatrix}
$$
<p>&nbsp;<br>

<p>We assume that the basis is orthogonal, that is </p>
<p>&nbsp;<br>
$$
\mathbf{v}_j^T\mathbf{v}_i = \delta_{ij}.
$$
<p>&nbsp;<br>

<p>An orthogonal or unitary transformation</p>
<p>&nbsp;<br>
$$
\mathbf{w}_i=\mathbf{U}\mathbf{v}_i,
$$
<p>&nbsp;<br>

<p>preserves the dot product and orthogonality since</p>
<p>&nbsp;<br>
$$
\mathbf{w}_j^T\mathbf{w}_i=(\mathbf{U}\mathbf{v}_j)^T\mathbf{U}\mathbf{v}_i=\mathbf{v}_j^T\mathbf{U}^T\mathbf{U}\mathbf{v}_i= \mathbf{v}_j^T\mathbf{v}_i = \delta_{ij}.
$$
<p>&nbsp;<br>

<p>This means that if the coefficients \( C_{p\lambda} \) belong to a unitary or orthogonal trasformation (using the Dirac bra-ket notation)</p>
<p>&nbsp;<br>
$$
\vert p\rangle  = \sum_{\lambda} C_{p\lambda}\vert\lambda\rangle,
$$
<p>&nbsp;<br>

<p>orthogonality is preserved, that is \( \langle \alpha \vert \beta\rangle = \delta_{\alpha\beta} \)
and \( \langle p \vert q\rangle = \delta_{pq} \). 
</p>

<p>This propertry is extremely useful when we build up a basis of many-body Stater determinant based states. </p>

<b>Note also that although a basis \( \vert \alpha\rangle \) contains an infinity of states, for practical calculations we have always to make some truncations.</b>

<p>Before we develop for example the Hartree-Fock equations, there is another very useful property of determinants that we will use both in connection with Hartree-Fock calculations and later shell-model calculations.  </p>

<p>Consider the following determinant</p>
<p>&nbsp;<br>
$$
\left| \begin{array}{cc} \alpha_1b_{11}+\alpha_2sb_{12}& a_{12}\\
                         \alpha_1b_{21}+\alpha_2b_{22}&a_{22}\end{array} \right|=\alpha_1\left|\begin{array}{cc} b_{11}& a_{12}\\
                         b_{21}&a_{22}\end{array} \right|+\alpha_2\left| \begin{array}{cc} b_{12}& a_{12}\\b_{22}&a_{22}\end{array} \right|
$$
<p>&nbsp;<br>

<p>We can generalize this to  an \( n\times n \) matrix and have </p>
<p>&nbsp;<br>
$$
\left| \begin{array}{cccccc} a_{11}& a_{12} & \dots & \sum_{k=1}^n c_k b_{1k} &\dots & a_{1n}\\
a_{21}& a_{22} & \dots & \sum_{k=1}^n c_k b_{2k} &\dots & a_{2n}\\
\dots & \dots & \dots & \dots & \dots & \dots \\
\dots & \dots & \dots & \dots & \dots & \dots \\
a_{n1}& a_{n2} & \dots & \sum_{k=1}^n c_k b_{nk} &\dots & a_{nn}\end{array} \right|=
\sum_{k=1}^n c_k\left| \begin{array}{cccccc} a_{11}& a_{12} & \dots &  b_{1k} &\dots & a_{1n}\\
a_{21}& a_{22} & \dots &  b_{2k} &\dots & a_{2n}\\
\dots & \dots & \dots & \dots & \dots & \dots\\
\dots & \dots & \dots & \dots & \dots & \dots\\
a_{n1}& a_{n2} & \dots &  b_{nk} &\dots & a_{nn}\end{array} \right| .
$$
<p>&nbsp;<br>

<p>This is a property we will use in our Hartree-Fock discussions. </p>

<p>We can generalize the previous results, now 
with all elements \( a_{ij} \)  being given as functions of 
linear combinations  of various coefficients \( c \) and elements \( b_{ij} \),
</p>
<p>&nbsp;<br>
$$
\left| \begin{array}{cccccc} \sum_{k=1}^n b_{1k}c_{k1}& \sum_{k=1}^n b_{1k}c_{k2} & \dots & \sum_{k=1}^n b_{1k}c_{kj}  &\dots & \sum_{k=1}^n b_{1k}c_{kn}\\
\sum_{k=1}^n b_{2k}c_{k1}& \sum_{k=1}^n b_{2k}c_{k2} & \dots & \sum_{k=1}^n b_{2k}c_{kj} &\dots & \sum_{k=1}^n b_{2k}c_{kn}\\
\dots & \dots & \dots & \dots & \dots & \dots \\
\dots & \dots & \dots & \dots & \dots &\dots \\
\sum_{k=1}^n b_{nk}c_{k1}& \sum_{k=1}^n b_{nk}c_{k2} & \dots & \sum_{k=1}^n b_{nk}c_{kj} &\dots & \sum_{k=1}^n b_{nk}c_{kn}\end{array} \right|=det(\mathbf{C})det(\mathbf{B}),
$$
<p>&nbsp;<br>

<p>where \( det(\mathbf{C}) \) and \( det(\mathbf{B}) \) are the determinants of \( n\times n \) matrices
with elements \( c_{ij} \) and \( b_{ij} \) respectively.  
This is a property we will use in our Hartree-Fock discussions. Convince yourself about the correctness of the above expression by setting \( n=2 \). 
</p>

<p>With our definition of the new basis in terms of an orthogonal basis we have</p>
<p>&nbsp;<br>
$$
\psi_p(x)  = \sum_{\lambda} C_{p\lambda}\phi_{\lambda}(x).
$$
<p>&nbsp;<br>

<p>If the coefficients \( C_{p\lambda} \) belong to an orthogonal or unitary matrix, the new basis
is also orthogonal. 
Our Slater determinant in the new basis \( \psi_p(x) \) is written as
</p>
<p>&nbsp;<br>
$$
\frac{1}{\sqrt{N!}}
\left| \begin{array}{ccccc} \psi_{p}(x_1)& \psi_{p}(x_2)& \dots & \dots & \psi_{p}(x_N)\\
                            \psi_{q}(x_1)&\psi_{q}(x_2)& \dots & \dots & \psi_{q}(x_N)\\  
                            \dots & \dots & \dots & \dots & \dots \\
                            \dots & \dots & \dots & \dots & \dots \\
                     \psi_{t}(x_1)&\psi_{t}(x_2)& \dots & \dots & \psi_{t}(x_N)\end{array} \right|=\frac{1}{\sqrt{N!}}
\left| \begin{array}{ccccc} \sum_{\lambda} C_{p\lambda}\phi_{\lambda}(x_1)& \sum_{\lambda} C_{p\lambda}\phi_{\lambda}(x_2)& \dots & \dots & \sum_{\lambda} C_{p\lambda}\phi_{\lambda}(x_N)\\
                            \sum_{\lambda} C_{q\lambda}\phi_{\lambda}(x_1)&\sum_{\lambda} C_{q\lambda}\phi_{\lambda}(x_2)& \dots & \dots & \sum_{\lambda} C_{q\lambda}\phi_{\lambda}(x_N)\\  
                            \dots & \dots & \dots & \dots & \dots \\
                            \dots & \dots & \dots & \dots & \dots \\
                     \sum_{\lambda} C_{t\lambda}\phi_{\lambda}(x_1)&\sum_{\lambda} C_{t\lambda}\phi_{\lambda}(x_2)& \dots & \dots & \sum_{\lambda} C_{t\lambda}\phi_{\lambda}(x_N)\end{array} \right|,
$$
<p>&nbsp;<br>

<p>which is nothing but \( det(\mathbf{C})det(\Phi) \), with \( det(\Phi) \) being the determinant given by the basis functions \( \phi_{\lambda}(x) \). </p>

<p>In our discussions hereafter we will use our definitions of single-particle states above and below the Fermi (\( F \)) level given by the labels
\( ijkl\dots \le F \) for so-called single-hole states and \( abcd\dots > F \) for so-called particle states.
For general single-particle states we employ the labels \( pqrs\dots \). 
</p>

<p>The energy functional is</p>
<p>&nbsp;<br>
$$
  E[\Phi] 
  = \sum_{\mu=1}^N \langle \mu | h | \mu \rangle +
  \frac{1}{2}\sum_{{\mu}=1}^N\sum_{{\nu}=1}^N \langle \mu\nu|\hat{v}|\mu\nu\rangle_{AS},
$$
<p>&nbsp;<br>

<p>we found the expression for the energy functional in terms of the basis function \( \phi_{\lambda}(\mathbf{r}) \). We then  varied the above energy functional with respect to the basis functions \( |\mu \rangle \). 
Now we are interested in defining a new basis defined in terms of
a chosen basis as defined in Eq.&nbsp;<a href="#mjx-eqn-12">(12)</a>. We can then rewrite the energy functional as
</p>
<p>&nbsp;<br>
$$
\begin{equation}
  E[\Phi^{New}] 
  = \sum_{i=1}^N \langle i | h | i \rangle +
  \frac{1}{2}\sum_{ij=1}^N\langle ij|\hat{v}|ij\rangle_{AS}, \tag{13}
\end{equation}
$$
<p>&nbsp;<br>

<p>where \( \Phi^{New} \) is the new Slater determinant defined by the new basis of Eq.&nbsp;<a href="#mjx-eqn-12">(12)</a>. </p>

<p>Using Eq.&nbsp;<a href="#mjx-eqn-12">(12)</a> we can rewrite Eq.&nbsp;<a href="#mjx-eqn-13">(13)</a> as </p>
<p>&nbsp;<br>
$$
\begin{equation}
  E[\Psi] 
  = \sum_{i=1}^N \sum_{\alpha\beta} C^*_{i\alpha}C_{i\beta}\langle \alpha | h | \beta \rangle +
  \frac{1}{2}\sum_{ij=1}^N\sum_{{\alpha\beta\gamma\delta}} C^*_{i\alpha}C^*_{j\beta}C_{i\gamma}C_{j\delta}\langle \alpha\beta|\hat{v}|\gamma\delta\rangle_{AS}. \tag{14}
\end{equation}
$$
<p>&nbsp;<br>
</section>



</div> <!-- class="slides" -->
</div> <!-- class="reveal" -->

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

  // Display navigation controls in the bottom right corner
  controls: true,

  // Display progress bar (below the horiz. slider)
  progress: true,

  // Display the page number of the current slide
  slideNumber: true,

  // Push each slide change to the browser history
  history: false,

  // Enable keyboard shortcuts for navigation
  keyboard: true,

  // Enable the slide overview mode
  overview: true,

  // Vertical centering of slides
  //center: true,
  center: false,

  // Enables touch navigation on devices with touch input
  touch: true,

  // Loop the presentation
  loop: false,

  // Change the presentation direction to be RTL
  rtl: false,

  // Turns fragments on and off globally
  fragments: true,

  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,

  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,

  // Stop auto-sliding after user input
  autoSlideStoppable: true,

  // Enable slide navigation via mouse wheel
  mouseWheel: false,

  // Hides the address bar on mobile devices
  hideAddressBar: true,

  // Opens links in an iframe preview overlay
  previewLinks: false,

  // Transition style
  transition: 'default', // default/cube/page/concave/zoom/linear/fade/none

  // Transition speed
  transitionSpeed: 'default', // default/fast/slow

  // Transition style for full page slide backgrounds
  backgroundTransition: 'default', // default/none/slide/concave/convex/zoom

  // Number of slides away from the current that are visible
  viewDistance: 3,

  // Parallax background image
    //parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

  // Parallax background size
  //parallaxBackgroundSize: '' // CSS syntax, e.g. "2100px 900px"

  theme: Reveal.getQueryHash().theme, // available themes are in reveal.js/css/theme
    transition: Reveal.getQueryHash().transition || 'none', // default/cube/page/concave/zoom/linear/none

});

Reveal.initialize({
  dependencies: [
      // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
      { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },

      // Interpret Markdown in <section> elements
      { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

      // Syntax highlight for <code> elements
      { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

      // Zoom in and out with Alt+click
      { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },

      // Speaker notes
      { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },

      // Remote control your reveal.js presentation using a touch device
      //{ src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } },

      // MathJax
      //{ src: 'reveal.js/plugin/math/math.js', async: true }
  ]
});

Reveal.initialize({

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 1170,  // original: 960,
  height: 700,

  // Factor of the display size that should remain empty around the content
  margin: 0.1,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 1.0

});
</script>

<!-- begin footer logo
<div style="position: absolute; bottom: 0px; left: 0; margin-left: 0px">
<img src="somelogo.png">
</div>
   end footer logo -->




</body>
</html>
